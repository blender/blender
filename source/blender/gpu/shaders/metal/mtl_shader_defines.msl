/* SPDX-FileCopyrightText: 2022 Blender Authors
 *
 * SPDX-License-Identifier: GPL-2.0-or-later */

/** Special header for mapping commonly defined tokens to API-specific variations.
 * Where possible, this will adhere closely to base GLSL, where semantics are the same.
 * However, host code shader code may need modifying to support types where necessary variations
 * exist between APIs but are not expressed through the source. (e.g. distinction between depth2d
 * and texture2d types in metal).
 */

/* Suppress unhelpful shader compiler warnings. */
#pragma clang diagnostic ignored "-Wunused-variable"
#pragma clang diagnostic ignored "-Wcomment"

/* Base instance with offsets. */
#define gpu_BaseInstance gl_BaseInstanceARB
#define gpu_InstanceIndex (gl_InstanceID + gpu_BaseInstance)

/* derivative signs. */
#define DFDX_SIGN 1.0
#define DFDY_SIGN 1.0

/* Type definitions. */
/* int implicitly cast to bool in MSL. */
using bool32_t = int32_t;
using vec3_1010102_Unorm = uint32_t;
using vec3_1010102_Inorm = int32_t;
/* GLSL types aliases. */
using vec2 = float2;
using vec3 = float3;
using vec4 = float4;
using mat2x2 = float2x2;
using mat2x3 = float2x3;
using mat2x4 = float2x4;
using mat3x2 = float3x2;
using mat3x3 = float3x3;
using mat3x4 = float3x4;
using mat4x2 = float4x2;
using mat4x3 = float4x3;
using mat4x4 = float4x4;
using mat2 = float2x2;
using mat3 = float3x3;
using mat4 = float4x4;
using ivec2 = int2;
using ivec3 = int3;
using ivec4 = int4;
using uvec2 = uint2;
using uvec3 = uint3;
using uvec4 = uint4;
using bvec2 = bool2;
using bvec3 = bool3;
using bvec4 = bool4;

/* Compute decorators. */
#define TG threadgroup
#define barrier() \
  threadgroup_barrier(mem_flags::mem_threadgroup | mem_flags::mem_device | mem_flags::mem_texture)

#ifdef MTL_USE_WORKGROUP_SIZE
/* Compute work-group size. */
struct constexp_uvec3 {
  /* Type union to cover all syntax accessors:
   * .x, .y, .z, .xy, .xyz
   * Swizzle types invalid.*/
  union {
    struct {
      uint x, y, z;
    };
    struct {
      uint2 xy;
    };
    uint3 xyz;
  };

  constexpr constexp_uvec3(uint _x, uint _y, uint _z) : x(_x), y(_y), z(_z) {}
  constexpr uint operator[](int i)
  {
    /* Note: Need to switch on each elem value as array accessor triggers
     * non-constant sizing error. This will be statically evaluated at compile time. */
    switch (i) {
      case 0:
        return x;
      case 1:
        return y;
      case 2:
        return z;
      default:
        return 0;
    }
  }
  constexpr inline operator uint3() const
  {
    return xyz;
  }
  constexpr inline operator uint2() const
  {
    return xy;
  }
  constexpr inline operator uint() const
  {
    return x;
  }
};

constexpr constexp_uvec3 __internal_workgroupsize_get()
{
  return constexp_uvec3(MTL_WORKGROUP_SIZE_X, MTL_WORKGROUP_SIZE_Y, MTL_WORKGROUP_SIZE_Z);
}

#  define gl_WorkGroupSize __internal_workgroupsize_get()
#endif

/** Shader atomics:
 * In order to emulate GLSL-style atomic operations, wherein variables can be used within atomic
 * operations, even if they are not explicitly declared atomic, we can cast the pointer to atomic,
 * to ensure that the load instruction follows atomic_load/store idioms.
 *
 * NOTE: We cannot hoist the address space into the template declaration, so these must be declared
 * for each relevant address space. */

/* Thread-group memory. */
template<typename T> T atomicMax(threadgroup T &mem, T data)
{
  return atomic_fetch_max_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicMin(threadgroup T &mem, T data)
{
  return atomic_fetch_min_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicAdd(threadgroup T &mem, T data)
{
  return atomic_fetch_add_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicSub(threadgroup T &mem, T data)
{
  return atomic_fetch_sub_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicAnd(threadgroup T &mem, T data)
{
  return atomic_fetch_and_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicOr(threadgroup T &mem, T data)
{
  return atomic_fetch_or_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicXor(threadgroup T &mem, T data)
{
  return atomic_fetch_xor_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicExchange(threadgroup T &mem, T data)
{
  return atomic_exchange_explicit((threadgroup _atomic<T> *)&mem, data, memory_order_relaxed);
}

/* Device memory. */
template<typename T> T atomicMax(device T &mem, T data)
{
  return atomic_fetch_max_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicMin(device T &mem, T data)
{
  return atomic_fetch_min_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicAdd(device T &mem, T data)
{
  return atomic_fetch_add_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicSub(device T &mem, T data)
{
  return atomic_fetch_sub_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicAnd(device T &mem, T data)
{
  return atomic_fetch_and_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicOr(device T &mem, T data)
{
  return atomic_fetch_or_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicXor(device T &mem, T data)
{
  return atomic_fetch_xor_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}
template<typename T> T atomicExchange(device T &mem, T data)
{
  return atomic_exchange_explicit((device _atomic<T> *)&mem, data, memory_order_relaxed);
}

/* Used to replace 'out' in function parameters with thread-local reference
 * shortened to avoid expanding the GLSL source string. */
#define THD thread
#define OUT(type, name, array) thread type(&name)[array]

/* Generate wrapper structs for combined texture and sampler type. */
#ifdef USE_ARGUMENT_BUFFER_FOR_SAMPLERS
#  define SAMPLER_DECLARATION constant sampler *samp;
#else
#  define SAMPLER_DECLARATION thread sampler *samp;
#endif

#define COMBINED_SAMPLER_TYPE(STRUCT_NAME, TEX_TYPE) \
  template<typename T, access A = access::sample> struct STRUCT_NAME { \
    thread TEX_TYPE<T, A> *texture; \
    SAMPLER_DECLARATION \
  }

/* If native texture atomics are unsupported, we instead utilize a custom type which wraps a
 * buffer-backed texture. This texture will always be a Texture2D, but will emulate access to
 * Texture3D and Texture2DArray by stacking layers.
 * Access pattern will be derived based on the source type. 2DArray and 3D atomic texture
 * support will require information on the size of each layer within the source 2D texture.
 *
 * A device pointer to the backing buffer will also be available for the atomic operations.
 * NOTE: For atomic ops, it will only be valid to use access::read_write.
 * We still need to use the wrapped type for access:sample, as texture2DArray and texture3D
 * will require access indirection.
 *
 * NOTE: Only type of UINT is valid, but full template provided to match syntax of standard
 * textures. */
#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS

template<typename T, access A = access::sample>
struct _mtl_combined_image_sampler_2d_atomic_fallback {
  thread texture2d<T, A> *texture;
  SAMPLER_DECLARATION
  device T *buffer;
  /* Aligned width matches the number of buffer elements in bytes_per_row. This may be greater than
   * the texture's native width to satisfy device alignment rules. We need to use the padded width
   * when writing to ensure the
   * correct writing location aligns with a given pixel location in the texture. */
  uint aligned_width;
};

template<typename T, access A = access::sample>
struct _mtl_combined_image_sampler_2d_array_atomic_fallback {
  thread texture2d<T, A> *texture;
  SAMPLER_DECLARATION
  device T *buffer;
  /* Aligned width matches the number of buffer elements in bytes_per_row. This may be greater than
   * the texture's native width to satisfy device alignment rules. We need to use the padded width
   * when writing to ensure the
   * correct writing location aligns with a given pixel location in the texture. */
  uint aligned_width;
  /* Texture size required to determine location offset of array layer with 2D texture space. */
  ushort3 texture_size;
};

template<typename T, access A = access::sample>
struct _mtl_combined_image_sampler_3d_atomic_fallback {
  thread texture2d<T, A> *texture;
  SAMPLER_DECLARATION
  device T *buffer;
  /* Aligned width matches the number of buffer elements in bytes_per_row. This may be greater than
   * the texture's native width to satisfy device alignment rules. We need to use the padded width
   * when writing to ensure the
   * correct writing location aligns with a given pixel location in the texture. */
  uint aligned_width;
  /* Texture size required to determine location offset of array layer with 2D texture space. */
  ushort3 texture_size;
};
#endif

/* Add any types as needed. */
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_1d, texture1d);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_1d_array, texture1d_array);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_2d, texture2d);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_depth_2d, depth2d);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_2d_array, texture2d_array);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_depth_2d_array, depth2d_array);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_3d, texture3d);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_buffer, texture_buffer);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_cube, texturecube);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_cube_array, texturecube_array);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_depth_cube, texturecube_array);
COMBINED_SAMPLER_TYPE(_mtl_combined_image_sampler_depth_cube_array, texturecube_array);

/* Sampler struct for argument buffer. */
#ifdef USE_ARGUMENT_BUFFER_FOR_SAMPLERS
struct SStruct {
  array<sampler, ARGUMENT_BUFFER_NUM_SAMPLERS> sampler_args [[id(0)]];
};
#endif

/* Samplers as function parameters. */
#define sampler1D thread _mtl_combined_image_sampler_1d<float>
#define sampler1DArray thread _mtl_combined_image_sampler_1d_array<float>
#define sampler2D thread _mtl_combined_image_sampler_2d<float>
#define depth2D thread _mtl_combined_image_sampler_depth_2d<float>
#define sampler2DArray thread _mtl_combined_image_sampler_2d_array<float>
#define sampler2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<float>
#define depth2DArray thread _mtl_combined_image_sampler_depth_2d_array<float>
#define depth2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<float>
#define sampler3D thread _mtl_combined_image_sampler_3d<float>
#define samplerBuffer thread _mtl_combined_image_sampler_buffer<float, access::read>
#define samplerCube thread _mtl_combined_image_sampler_cube<float>
#define samplerCubeArray thread _mtl_combined_image_sampler_cube_array<float>

#define usampler1D thread _mtl_combined_image_sampler_1d<uint>
#define usampler1DArray thread _mtl_combined_image_sampler_1d_array<uint>
#define usampler2D thread _mtl_combined_image_sampler_2d<uint>
#define udepth2D thread _mtl_combined_image_sampler_depth_2d<uint>
#define usampler2DArray thread _mtl_combined_image_sampler_2d_array<uint>
#define usampler2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<uint>
#define udepth2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<uint>
#define usampler3D thread _mtl_combined_image_sampler_3d<uint>
#define usamplerBuffer thread _mtl_combined_image_sampler_buffer<uint, access::read>
#define usamplerCube thread _mtl_combined_image_sampler_cube<uint>
#define usamplerCubeArray thread _mtl_combined_image_sampler_cube_array<uint>

#define isampler1D thread _mtl_combined_image_sampler_1d<int>
#define isampler1DArray thread _mtl_combined_image_sampler_1d_array<int>
#define isampler2D thread _mtl_combined_image_sampler_2d<int>
#define idepth2D thread _mtl_combined_image_sampler_depth_2d<int>
#define isampler2DArray thread _mtl_combined_image_sampler_2d_array<int>
#define isampler2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<int>
#define idepth2DArrayShadow thread _mtl_combined_image_sampler_depth_2d_array<int>
#define isampler3D thread _mtl_combined_image_sampler_3d<int>
#define isamplerBuffer thread _mtl_combined_image_sampler_buffer<int, access::read>
#define isamplerCube thread _mtl_combined_image_sampler_cube<int>
#define isamplerCubeArray thread _mtl_combined_image_sampler_cube_array<int>

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
/* If texture atomics are unsupported, map atomic types to internal atomic fallback type. */
#  define usampler2DArrayAtomic _mtl_combined_image_sampler_2d_array_atomic_fallback<uint>
#  define usampler2DAtomic _mtl_combined_image_sampler_2d_atomic_fallback<uint>
#  define usampler3DAtomic _mtl_combined_image_sampler_3d_atomic_fallback<uint>
#  define isampler2DArrayAtomic _mtl_combined_image_sampler_2d_array_atomic_fallback<int>
#  define isampler2DAtomic _mtl_combined_image_sampler_2d_atomic_fallback<int>
#  define isampler3DAtomic _mtl_combined_image_sampler_3d_atomic_fallback<int>
#else
#  define usampler2DArrayAtomic usampler2DArray
#  define usampler2DAtomic usampler2D
#  define usampler3DAtomic usampler3D
#  define isampler2DArrayAtomic isampler2DArray
#  define isampler2DAtomic isampler2D
#  define isampler3DAtomic isampler3D
#endif

/* Vector accessor aliases. */
#define st xy

/* Texture functions. */
#define texelFetch _texelFetch_internal
#define texelFetchOffset(__tex, __texel, __lod, __offset) \
  _texelFetch_internal(__tex, __texel, __lod, __offset)
#define imageLoad(__image, __coord) _texelFetch_internal(__image, __coord, 0)
#define imageLoadFast(__image, __coord) _texelFetch_internal_fast(__image, __coord, 0)
#define texture2(__tex, __uv) _texture_internal_samp(__tex, __uv)
#define texture3(__tex, __uv, _bias) _texture_internal_bias(__tex, __uv, bias(float(_bias)))
#define textureLod(__tex, __uv, __lod) _texture_internal_level(__tex, __uv, level(float(__lod)))
#define textureLodOffset(__tex, __uv, __lod, __offset) \
  _texture_internal_level(__tex, __uv, level(float(__lod)), __offset)
#define textureGather2(__tex, __uv) _texture_gather_internal(__tex, __uv, 0)
#define textureGather3(__tex, __uv, __comp) _texture_gather_internal(__tex, __uv, __comp)
#define textureGatherOffset(__tex, __offset, __uv, __comp) \
  _texture_gather_internal(__tex, __uv, __comp, __offset)
#define textureGrad(__tex, __uv, __dpdx, __dpdy) \
  _texture_grad_internal(__tex, __uv, __dpdx, __dpdy)

#define TEXURE_MACRO(_1, _2, _3, TEXNAME, ...) TEXNAME
#define texture(...) TEXURE_MACRO(__VA_ARGS__, texture3, texture2)(__VA_ARGS__)
#define textureGather(...) TEXURE_MACRO(__VA_ARGS__, textureGather3, textureGather2)(__VA_ARGS__)

/* Texture-write functions. */
#define imageStore(_tex, _coord, _value) _texture_write_internal(_tex, _coord, _value)
#define imageStoreFast(_tex, _coord, _value) _texture_write_internal_fast(_tex, _coord, _value)

/* Texture synchronization functions. */
#define imageFence(image) image.texture->fence()

/* Singular return values from texture functions of type DEPTH are often indexed with either .r or
 * .x. This is a lightweight wrapper type for handling this syntax. */
union _msl_return_float {
  float r;
  float x;
  inline operator float() const
  {
    return r;
  }
};

/* Add custom texture sampling/reading routines for each type to account for special return cases,
 * e.g. returning a float with an r parameter Note: Cannot use template specialization for input
 * type, as return types are specific to the signature of 'tex'. */

/* Use point sampler instead of texture read to benefit from texture caching and reduce branching
 * through removal of bounds tests, as these are handled by the sample operation. */
constexpr sampler _point_sample_(address::clamp_to_zero, filter::nearest, coord::pixel);

/* Texture Read via point sampling.
 * NOTE: These templates will evaluate first for texture resources bound with sample. */
template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_1d<S, access::sample> tex,
                                      T texel,
                                      uint lod = 0)
{
  return tex.texture->sample(_point_sample_, float(texel));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_1d<S, access::sample> tex,
                                      T texel,
                                      uint lod,
                                      T offset)
{
  return tex.texture->sample(_point_sample_, float(texel + offset));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_1d_array<S, access::sample> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0, 0))
{
  return tex.texture->sample(_point_sample_, float(texel.x + offset.x), uint(texel.y + offset.y));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_2d<S, access::sample> tex,
                                      vec<T, 2> texel,
                                      uint lod,
                                      vec<T, 2> offset = vec<T, 2>(0))
{
  return tex.texture->sample(_point_sample_, float2(texel.xy + offset.xy), level(lod));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_2d_array<S, access::sample> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->sample(
      _point_sample_, float2(texel.xy + offset.xy), uint(texel.z + offset.z), level(lod));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_3d<S, access::sample> tex,
                                      vec<T, 3> texel,
                                      uint lod,
                                      vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->sample(_point_sample_, float3(texel.xyz + offset.xyz), level(lod));
}

template<typename T>
inline _msl_return_float _texelFetch_internal(
    thread _mtl_combined_image_sampler_depth_2d<float, access::sample> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0))
{
  _msl_return_float fl = {
      tex.texture->sample(_point_sample_, float2(texel.xy + offset.xy), level(lod))};
  return fl;
}

template<typename S, typename T>
inline vec<S, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_2d_array<S, access::sample> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->sample(
      _point_sample_, float2(texel.xy + offset.xy), uint(texel.z + offset.z), level(lod));
}

/* Texture Read via read operation. Required by compute/image-bindings. */
template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                      T texel,
                                      uint lod = 0)
{
  float w = tex.texture->get_width();
  if (texel >= 0 && texel < w) {
    return tex.texture->read(uint(texel));
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                           T texel,
                                           uint lod = 0)
{
  return tex.texture->read(uint(texel));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    const thread _mtl_combined_image_sampler_buffer<S, access::read> tex, T texel, uint lod = 0)
{
  float w = tex.texture->get_width();
  if (texel >= 0 && texel < w) {
    return tex.texture->read(uint(texel));
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal_fast(
    const thread _mtl_combined_image_sampler_buffer<S, access::read> tex, T texel, uint lod = 0)
{
  return tex.texture->read(uint(texel));
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                      T texel,
                                      uint lod,
                                      T offset)
{
  float w = tex.texture->get_width();
  if ((texel + offset) >= 0 && (texel + offset) < w) {
    /* LODs not supported for 1d textures. This must be zero. */
    return tex.texture->read(uint(texel + offset), 0);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                           T texel,
                                           uint lod,
                                           T offset)
{
  /* LODs not supported for 1d textures. This must be zero. */
  return tex.texture->read(uint(texel + offset), 0);
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                      vec<T, 2> texel,
                                      uint lod,
                                      vec<T, 2> offset = vec<T, 2>(0, 0))
{

  float w = tex.texture->get_width();
  float h = tex.texture->get_array_size();
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h)
  {
    /* LODs not supported for 1d textures. This must be zero. */
    return tex.texture->read(uint(texel.x + offset.x), uint(texel.y + offset.y), 0);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                           vec<T, 2> texel,
                                           uint lod,
                                           vec<T, 2> offset = vec<T, 2>(0, 0))
{
  /* LODs not supported for 1d textures. This must be zero. */
  return tex.texture->read(uint(texel.x + offset.x), uint(texel.y + offset.y), 0);
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                      vec<T, 2> texel,
                                      uint lod,
                                      vec<T, 2> offset = vec<T, 2>(0))
{

  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h)
  {
    return tex.texture->read(uint2(texel + offset), lod);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                           vec<T, 2> texel,
                                           uint lod,
                                           vec<T, 2> offset = vec<T, 2>(0))
{
  return tex.texture->read(uint2(texel + offset), lod);
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                      vec<T, 3> texel,
                                      uint lod,
                                      vec<T, 3> offset = vec<T, 3>(0))
{
  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  float d = tex.texture->get_array_size();
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h && (texel.z + offset.z) >= 0 && (texel.z + offset.z) < d)
  {
    return tex.texture->read(uint2(texel.xy + offset.xy), uint(texel.z + offset.z), lod);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                           vec<T, 3> texel,
                                           uint lod,
                                           vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->read(uint2(texel.xy + offset.xy), uint(texel.z + offset.z), lod);
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                      vec<T, 3> texel,
                                      uint lod,
                                      vec<T, 3> offset = vec<T, 3>(0))
{

  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  float d = tex.texture->get_depth() >> lod;
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h && (texel.z + offset.z) >= 0 && (texel.z + offset.z) < d)
  {
    return tex.texture->read(uint3(texel + offset), lod);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal_fast(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                           vec<T, 3> texel,
                                           uint lod,
                                           vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->read(uint3(texel + offset), lod);
}

template<typename T, access A>
inline _msl_return_float _texelFetch_internal(
    thread _mtl_combined_image_sampler_depth_2d<float, A> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0))
{

  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h)
  {
    _msl_return_float fl = {tex.texture->read(uint2(texel + offset), lod)};
    return fl;
  }
  else {
    _msl_return_float fl = {0};
    return fl;
  }
}

template<typename T, access A>
inline _msl_return_float _texelFetch_internal_fast(
    thread _mtl_combined_image_sampler_depth_2d<float, A> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0))
{
  _msl_return_float fl = {tex.texture->read(uint2(texel + offset), lod)};
  return fl;
}

template<typename S, typename T, access A>
inline vec<S, 4> _texture_internal_samp(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                        vec<T, 3> texel,
                                        uint lod,
                                        vec<T, 3> offset = vec<T, 3>(0))
{

  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  float d = tex.texture->get_array_size();
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h && (texel.z + offset.z) >= 0 && (texel.z + offset.z) < d)
  {
    return tex.texture->read(uint2(texel.xy + offset.xy), uint(texel.z + offset.z), lod);
  }
  else {
    return vec<S, 4>(0);
  }
}

/* Sample. */
template<typename T>
inline vec<T, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_1d<T, access::sample> tex, float u)
{
  return tex.texture->sample(*tex.samp, u);
}

inline float4 _texture_internal_samp(
    thread _mtl_combined_image_sampler_1d_array<float, access::sample> tex, float2 ua)
{
  return tex.texture->sample(*tex.samp, ua.x, uint(ua.y));
}

inline int4 _texture_internal_samp(thread _mtl_combined_image_sampler_2d<int, access::sample> tex,
                                   float2 uv)
{
  return tex.texture->sample(*tex.samp, uv);
}

inline uint4 _texture_internal_samp(
    thread _mtl_combined_image_sampler_2d<uint, access::sample> tex, float2 uv)
{
  return tex.texture->sample(*tex.samp, uv);
}

inline float4 _texture_internal_samp(
    thread _mtl_combined_image_sampler_2d<float, access::sample> tex, float2 uv)
{
  return tex.texture->sample(*tex.samp, uv);
}

inline _msl_return_float _texture_internal_samp(
    thread _mtl_combined_image_sampler_depth_2d<float, access::sample> tex, float2 uv)
{
  _msl_return_float fl = {tex.texture->sample(*tex.samp, uv)};
  return fl;
}

template<typename T>
inline vec<T, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_3d<T, access::sample> tex, float3 uvw)
{
  return tex.texture->sample(*tex.samp, uvw);
}

template<typename T>
inline vec<T, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_2d_array<T, access::sample> tex, float3 uva)
{
  return tex.texture->sample(*tex.samp, uva.xy, uint(uva.z));
}

inline _msl_return_float _texture_internal_samp(
    thread _mtl_combined_image_sampler_depth_2d_array<float, access::sample> tex, float3 uva)
{
  _msl_return_float fl = {tex.texture->sample(*tex.samp, uva.xy, uint(uva.z))};
  return fl;
}

inline _msl_return_float _texture_internal_samp(
    thread _mtl_combined_image_sampler_depth_2d_array<float, access::sample> tex, float4 uvac)
{
  _msl_return_float fl = {
      tex.texture->sample_compare(*tex.samp, uvac.xy, uint(uvac.z), uvac.w, level(0))};
  return fl;
}

template<typename T>
inline vec<T, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_cube<T, access::sample> tex, float3 uvs)
{
  return tex.texture->sample(*tex.samp, uvs.xyz);
}

template<typename T>
inline vec<T, 4> _texture_internal_samp(
    thread _mtl_combined_image_sampler_cube_array<T, access::sample> tex, float4 coord_a)
{
  return tex.texture->sample(*tex.samp, coord_a.xyz, uint(coord_a.w));
}

/* Sample Level. */
template<typename T>
inline vec<T, 4> _texture_internal_level(
    thread _mtl_combined_image_sampler_1d<T, access::sample> tex,
    float u,
    level options,
    int offset = 0)
{
  /* LODs not supported for 1d textures. This must be zero. */
  return tex.texture->sample(*tex.samp, u);
}

inline float4 _texture_internal_level(
    thread _mtl_combined_image_sampler_1d_array<float, access::sample> tex,
    float2 ua,
    level options,
    int offset = 0)
{
  /* LODs not supported for 1d textures. This must be zero. */
  return tex.texture->sample(*tex.samp, ua.x, uint(ua.y));
}

inline int4 _texture_internal_level(thread _mtl_combined_image_sampler_2d<int, access::sample> tex,
                                    float2 uv,
                                    level options,
                                    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uv, options, offset);
}

inline uint4 _texture_internal_level(
    thread _mtl_combined_image_sampler_2d<uint, access::sample> tex,
    float2 uv,
    level options,
    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uv, options, offset);
}

inline float4 _texture_internal_level(
    thread _mtl_combined_image_sampler_2d<float, access::sample> tex,
    float2 uv,
    level options,
    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uv, options, offset);
}

inline _msl_return_float _texture_internal_level(
    thread _mtl_combined_image_sampler_depth_2d<float, access::sample> tex,
    float2 uv,
    level options,
    int2 offset = int2(0))
{
  _msl_return_float fl = {tex.texture->sample(*tex.samp, uv, options, offset)};
  return fl;
}

template<typename T>
inline vec<T, 4> _texture_internal_level(
    thread _mtl_combined_image_sampler_3d<T, access::sample> tex,
    float3 uvw,
    level options = level(0),
    int3 offset = int3(0))
{
  return tex.texture->sample(*tex.samp, uvw, options, offset);
}

template<typename T>
inline vec<T, 4> _texture_internal_level(
    thread _mtl_combined_image_sampler_2d_array<T, access::sample> tex,
    float3 uva,
    level options = level(0),
    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uva.xy, uint(uva.z), options, offset);
}

inline _msl_return_float _texture_internal_level(
    thread _mtl_combined_image_sampler_depth_2d_array<float, access::sample> tex,
    float3 uva,
    level options = level(0),
    int2 offset = int2(0))
{
  _msl_return_float fl = {tex.texture->sample(*tex.samp, uva.xy, uint(uva.z), options, offset)};
  return fl;
}

inline _msl_return_float _texture_internal_level(
    thread _mtl_combined_image_sampler_depth_2d_array<float, access::sample> tex,
    float4 uvac,
    level options = level(0),
    int2 offset = int2(0))
{
  _msl_return_float fl = {
      tex.texture->sample_compare(*tex.samp, uvac.xy, uint(uvac.z), uvac.w, level(0), offset)};
  return fl;
}

template<typename T>
inline vec<T, 4> _texture_internal_level(
    thread _mtl_combined_image_sampler_cube<T, access::sample> tex,
    float3 uvs,
    level options = level(0),
    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uvs.xyz, options);
}

template<typename T>
inline vec<T, 4> _texture_internal_level(
    thread _mtl_combined_image_sampler_cube_array<T, access::sample> tex,
    float4 coord_a,
    level options = level(0),
    int3 offset = int3(0))
{
  return tex.texture->sample(*tex.samp, coord_a.xyz, uint(coord_a.w), options);
}

/* Sample Bias. */
template<typename T>
inline vec<T, 4> _texture_internal_bias(
    thread _mtl_combined_image_sampler_1d<T, access::sample> tex,
    float u,
    bias options = bias(0.0),
    int offset = 0)
{
  return tex.texture->sample(*tex.samp, u);
}

inline float4 _texture_internal_bias(
    thread _mtl_combined_image_sampler_2d<float, access::sample> tex,
    float2 uv,
    bias options = bias(0.0),
    int2 offset = int2(0))
{
  return tex.texture->sample(*tex.samp, uv, options, offset);
}

inline _msl_return_float _texture_internal_bias(
    thread _mtl_combined_image_sampler_depth_2d<float, access::sample> tex,
    float2 uv,
    bias options = bias(0),
    int2 offset = int2(0))
{
  _msl_return_float fl = {tex.texture->sample(*tex.samp, uv, options, offset)};
  return fl;
}

/* Texture Gather. */
component int_to_component(const int comp)
{
  switch (comp) {
    default:
    case 0:
      return component::x;
    case 1:
      return component::y;
    case 2:
      return component::z;
    case 3:
      return component::w;
  }
  return component::x;
}

inline float4 _texture_gather_internal(
    thread _mtl_combined_image_sampler_depth_2d<float, access::sample> tex,
    float2 uv,
    const int comp = 0,
    int2 offset = int2(0))
{
  return tex.texture->gather(*tex.samp, uv, offset);
}

inline float4 _texture_gather_internal(
    thread _mtl_combined_image_sampler_depth_2d_array<float, access::sample> tex,
    float3 uva,
    const int comp = 0,
    int2 offset = int2(0))
{
  return tex.texture->gather(*tex.samp, uva.xy, uint(uva.z), offset);
}

template<typename T>
inline vec<T, 4> _texture_gather_internal(
    thread _mtl_combined_image_sampler_2d<T, access::sample> tex,
    float2 uv,
    const int comp = 0,
    int2 offset = int2(0))
{
  return tex.texture->gather(*tex.samp, uv, offset);
}

template<typename T>
inline vec<T, 4> _texture_gather_internal(
    thread _mtl_combined_image_sampler_2d_array<T, access::sample> tex,
    float3 uva,
    const int comp = 0,
    int2 offset = int2(0))
{
  return tex.texture->gather(*tex.samp, uva.xy, uint(uva.z), offset);
}

/* Texture Grad. */
inline float4 _texture_grad_internal(
    thread _mtl_combined_image_sampler_2d<float, access::sample> tex,
    float2 uv,
    float2 dpdx,
    float2 dpdy)
{
  return tex.texture->sample(*tex.samp, uv, gradient2d(dpdx, dpdy));
}

inline float4 _texture_grad_internal(
    thread _mtl_combined_image_sampler_2d_array<float, access::sample> tex,
    float3 uva,
    float2 dpdx,
    float2 dpdy)
{
  return tex.texture->sample(*tex.samp, uva.xy, uint(uva.z), gradient2d(dpdx, dpdy));
}

inline float4 _texture_grad_internal(
    thread _mtl_combined_image_sampler_3d<float, access::sample> tex,
    float3 uvw,
    float3 dpdx,
    float3 dpdy)
{
  return tex.texture->sample(*tex.samp, uvw, gradient3d(dpdx, dpdy));
}

/* Texture write support. */
template<typename S, typename T, access A>
inline void _texture_write_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                    T _coord,
                                    vec<S, 4> value)
{
  float w = tex.texture->get_width();
  if (_coord >= 0 && _coord < w) {
    tex.texture->write(value, uint(_coord));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                         T _coord,
                                         vec<S, 4> value)
{
  tex.texture->write(value, uint(_coord));
}

template<typename S, typename T, access A>
inline void _texture_write_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                    T _coord,
                                    vec<S, 4> value)
{
  float w = tex.texture->get_width();
  float h = tex.texture->get_height();
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h) {
    tex.texture->write(value, uint2(_coord.xy));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                         T _coord,
                                         vec<S, 4> value)
{
  tex.texture->write(value, uint2(_coord.xy));
}

template<typename S, typename T, access A>
inline void _texture_write_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                    T _coord,
                                    vec<S, 4> value)
{
  float w = tex.texture->get_width();
  float h = tex.texture->get_height();
  float d = tex.texture->get_array_size();
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h && _coord.z >= 0 &&
      _coord.z < d)
  {
    tex.texture->write(value, uint2(_coord.xy), _coord.z);
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                         T _coord,
                                         vec<S, 4> value)
{
  tex.texture->write(value, uint2(_coord.xy), _coord.z);
}

template<typename S, typename T, access A>
inline void _texture_write_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                    T _coord,
                                    vec<S, 4> value)
{
  float w = tex.texture->get_width();
  float h = tex.texture->get_height();
  float d = tex.texture->get_depth();
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h && _coord.z >= 0 &&
      _coord.z < d)
  {
    tex.texture->write(value, uint3(_coord.xyz));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                         T _coord,
                                         vec<S, 4> value)
{
  tex.texture->write(value, uint3(_coord.xyz));
}

/* Texture atomic operations are only supported in Metal 3.1 and onward (macOS 14.0 Sonoma). */
#ifdef MTL_SUPPORTS_TEXTURE_ATOMICS

/* Image atomic operations. */
#  define imageAtomicMin(tex, coord, data) _texture_image_atomic_min_internal(tex, coord, data)
#  define imageAtomicAdd(tex, coord, data) _texture_image_atomic_add_internal(tex, coord, data)
#  define imageAtomicExchange(tex, coord, data) \
    _texture_image_atomic_exchange_internal(tex, coord, data)
#  define imageAtomicXor(tex, coord, data) _texture_image_atomic_xor_internal(tex, coord, data)
#  define imageAtomicOr(tex, coord, data) _texture_image_atomic_or_internal(tex, coord, data)

/* Atomic OR. */
template<typename S, access A>
S _texture_image_atomic_or_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                    int coord,
                                    S data)
{
  return tex.texture->atomic_fetch_or(uint(coord), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_or_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                    int2 coord,
                                    S data)
{
  return tex.texture->atomic_fetch_or(uint(coord.x), uint(coord.y), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_or_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                    int2 coord,
                                    S data)
{
  return tex.texture->atomic_fetch_or(uint2(coord.xy), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_or_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                    int3 coord,
                                    S data)
{
  return tex.texture->atomic_fetch_or(uint2(coord.xy), uint(coord.z), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_or_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                    int3 coord,
                                    S data)
{
  return tex.texture->atomic_fetch_or(uint3(coord), vec<S, 4>(data)).x;
}

/* Atomic XOR. */
template<typename S, access A>
S _texture_image_atomic_xor_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                     int coord,
                                     S data)
{
  return tex.texture->atomic_fetch_xor(uint(coord), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_xor_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_xor(uint(coord.x), uint(coord.y), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_xor_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_xor(uint2(coord.xy), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_xor_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_xor(uint2(coord.xy), uint(coord.z), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_xor_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_xor(uint3(coord), vec<S, 4>(data)).x;
}

/* Atomic Min. */
template<typename S, access A>
S _texture_image_atomic_min_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                     int coord,
                                     S data)
{
  return tex.texture->atomic_fetch_min(uint(coord), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_min_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_min(uint(coord.x), uint(coord.y), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_min_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_min(uint2(coord.xy), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_min_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_min(uint2(coord.xy), uint(coord.z), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_min_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_min(uint3(coord), vec<S, 4>(data)).x;
}

/* Atomic Add. */
template<typename S, access A>
S _texture_image_atomic_add_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                     int coord,
                                     S data)
{
  return tex.texture->atomic_fetch_add(uint(coord), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_add_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_add(uint(coord.x), uint(coord.y), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_add_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                     int2 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_add(uint2(coord.xy), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_add_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_add(uint2(coord.xy), uint(coord.z), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_add_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                     int3 coord,
                                     S data)
{
  return tex.texture->atomic_fetch_add(uint3(coord), vec<S, 4>(data)).x;
}

/* Atomic Exchange. */
template<typename S, access A>
S _texture_image_atomic_exchange_internal(thread _mtl_combined_image_sampler_1d<S, A> tex,
                                          int coord,
                                          S data)
{
  return tex.texture->atomic_exchange(uint(coord), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal(thread _mtl_combined_image_sampler_1d_array<S, A> tex,
                                          int2 coord,
                                          S data)
{
  return tex.texture->atomic_exchange(uint(coord.x), uint(coord.y), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal(thread _mtl_combined_image_sampler_2d<S, A> tex,
                                          int2 coord,
                                          S data)
{
  return tex.texture->atomic_exchange(uint2(coord.xy), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal(thread _mtl_combined_image_sampler_2d_array<S, A> tex,
                                          int3 coord,
                                          S data)
{
  return tex.texture->atomic_exchange(uint2(coord.xy), uint(coord.z), vec<S, 4>(data)).x;
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal(thread _mtl_combined_image_sampler_3d<S, A> tex,
                                          int3 coord,
                                          S data)
{
  return tex.texture->atomic_exchange(uint3(coord), vec<S, 4>(data)).x;
}

#else

/**
 * Texture atomic fallback function entry points.
 * NOTE: When texture atomics are unsupported, the wrapped type contains a buffer-backed 2D
 * texture. Atomic operations happen directly on the underlying buffer, and texture coordinates are
 * remapped into 2D texture space from 2D Array or 3D texture coordinates.
 */

/* Image atomic operations. */
#  define imageAtomicMin(tex, coord, data) \
    _texture_image_atomic_min_internal_fallback(tex, coord, data)
#  define imageAtomicAdd(tex, coord, data) \
    _texture_image_atomic_add_internal_fallack(tex, coord, data)
#  define imageAtomicExchange(tex, coord, data) \
    _texture_image_atomic_exchange_internal_fallack(tex, coord, data)
#  define imageAtomicXor(tex, coord, data) \
    _texture_image_atomic_xor_internal_fallack(tex, coord, data)
#  define imageAtomicOr(tex, coord, data) \
    _texture_image_atomic_or_internal_fallack(tex, coord, data)

/** Pixel address location remapping. */

/* Map 2D/3D texture coordinate into a linear pixel ID. */
template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex,
                            uint2 coord)
{
  return (coord.x + coord.y * uint(tex.texture->get_width()));
}
template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex,
                            int2 coord)
{
  return tex_coord_to_linear_px(tex, uint2(coord));
}

template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
                            uint3 coord)
{
  return (coord.x + coord.y * tex.texture_size.x +
          coord.z * (tex.texture_size.x * tex.texture_size.y));
}
template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
                            int3 coord)
{
  return tex_coord_to_linear_px(tex, uint3(coord));
}

template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
                            uint3 coord)
{
  return (coord.x + coord.y * tex.texture_size.x +
          coord.z * (tex.texture_size.x * tex.texture_size.y));
}
template<typename S, access A>
uint tex_coord_to_linear_px(thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
                            int3 coord)
{
  return tex_coord_to_linear_px(tex, uint3(coord));
}

/* Map 3D texture coordinate into 2D texture space. */
template<typename S, access A>
uint2 tex_coord_3d_to_2d(thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
                         uint3 coord)
{
  uint linear_id = tex_coord_to_linear_px(tex, coord);
  uint tex_full_w = uint(tex.texture->get_width());
  uint2 out_2dcoord;
  out_2dcoord.y = linear_id / tex_full_w;
  out_2dcoord.x = linear_id - (out_2dcoord.y * tex_full_w);
  return out_2dcoord;
}

template<typename S, access A>
uint2 tex_coord_3d_to_2d(thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
                         uint3 coord)
{
  uint linear_id = tex_coord_to_linear_px(tex, coord);
  uint tex_full_w = uint(tex.texture->get_width());
  uint2 out_2dcoord;
  out_2dcoord.y = linear_id / tex_full_w;
  out_2dcoord.x = linear_id - (out_2dcoord.y * tex_full_w);
  return out_2dcoord;
}

template<int N> bool in_range(vec<int, N> value, vec<int, N> min, vec<int, N> max)
{
  return (all(value >= min) && all(value < max));
}

/* Map 2D/3D texture coordinate into buffer index, accounting for padded row widths. */
template<typename S, access A>
uint tex_coord_to_linear_buffer_id(thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex,
                                   uint2 coord)
{
  return (coord.x + coord.y * uint(tex.aligned_width));
}
template<typename S, access A>
uint tex_coord_to_linear_buffer_id(thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex,
                                   int2 coord)
{
  return tex_coord_to_linear_buffer_id(tex, uint2(coord));
}

template<typename S, access A>
uint tex_coord_to_linear_buffer_id(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, uint3 coord)
{
  uint2 coord2d = tex_coord_3d_to_2d(tex, coord);
  return (coord2d.x + coord2d.y * uint(tex.aligned_width));
}
template<typename S, access A>
uint tex_coord_to_linear_buffer_id(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord)
{
  return tex_coord_to_linear_buffer_id(tex, uint3(coord));
}

template<typename S, access A>
uint tex_coord_to_linear_buffer_id(thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
                                   uint3 coord)
{
  uint2 coord2d = tex_coord_3d_to_2d(tex, coord);
  return (coord2d.x + coord2d.y * uint(tex.aligned_width));
}
template<typename S, access A>
uint tex_coord_to_linear_buffer_id(thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
                                   int3 coord)
{
  return tex_coord_to_linear_buffer_id(tex, uint3(coord));
}

/* imageAtomicMin. */

template<typename S, access A>
S _texture_image_atomic_min_internal_fallback(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, int2 coord, S data)
{
  if (!in_range(coord.xy, int2(0, 0), int2(tex.texture->get_width(0), tex.texture->get_height(0))))
  {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicMin(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_min_internal_fallback(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicMin(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_min_internal_fallback(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicMin(tex.buffer[linear_id], data);
}

/* imageAtomicAdd. */
template<typename S, access A>
S _texture_image_atomic_add_internal_fallack(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, int2 coord, S data)
{
  if (!in_range(coord.xy, int2(0, 0), int2(tex.texture->get_width(0), tex.texture->get_height(0))))
  {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicAdd(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_add_internal_fallack(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicAdd(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_add_internal_fallack(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicAdd(tex.buffer[linear_id], data);
}

/* imageAtomicExchange. */
template<typename S, access A>
S _texture_image_atomic_exchange_internal_fallack(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, int2 coord, S data)
{
  if (!in_range(coord.xy, int2(0, 0), int2(tex.texture->get_width(0), tex.texture->get_height(0))))
  {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicExchange(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal_fallack(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicExchange(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_exchange_internal_fallack(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicExchange(tex.buffer[linear_id], data);
}

/* imageAtomicXor. */
template<typename S, access A>
S _texture_image_atomic_xor_internal_fallack(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, int2 coord, S data)
{
  if (!in_range(coord.xy, int2(0, 0), int2(tex.texture->get_width(0), tex.texture->get_height(0))))
  {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicXor(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_xor_internal_fallack(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicXor(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_xor_internal_fallack(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicXor(tex.buffer[linear_id], data);
}

/* imageAtomicOr. */
template<typename S, access A>
S _texture_image_atomic_or_internal_fallack(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, int2 coord, S data)
{
  if (!in_range(coord.xy, int2(0, 0), int2(tex.texture->get_width(0), tex.texture->get_height(0))))
  {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicOr(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_or_internal_fallack(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicOr(tex.buffer[linear_id], data);
}

template<typename S, access A>
S _texture_image_atomic_or_internal_fallack(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, int3 coord, S data)
{
  if (!in_range(coord.xyz, int3(0, 0, 0), int3(tex.texture_size.xyz))) {
    return S(0);
  }
  uint linear_id = tex_coord_to_linear_buffer_id(tex, coord);
  return atomicOr(tex.buffer[linear_id], data);
}

/** Texture sampling, reading and writing functions with layer mapping. */

/* Texel Fetch. */
template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, access::sample> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0))
{
  return tex.texture->sample(_point_sample_, float2(texel.xy + offset.xy), level(lod));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, access::sample> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->sample(
      _point_sample_, float2(tex_coord_3d_to_2d(tex, uint3(texel + offset))), level(lod));
}

template<typename S, typename T>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, access::sample> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->sample(
      _point_sample_, float2(tex_coord_3d_to_2d(tex, uint3(texel + offset))), level(lod));
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex,
    vec<T, 2> texel,
    uint lod,
    vec<T, 2> offset = vec<T, 2>(0))
{

  float w = tex.texture->get_width() >> lod;
  float h = tex.texture->get_height() >> lod;
  if ((texel.x + offset.x) >= 0 && (texel.x + offset.x) < w && (texel.y + offset.y) >= 0 &&
      (texel.y + offset.y) < h)
  {
    return tex.texture->read(uint2(texel + offset), lod);
  }
  else {
    return vec<S, 4>(0);
  }
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->read(tex_coord_3d_to_2d(tex, uint3(texel + offset)), lod);
}

template<typename S, typename T, access A>
inline vec<S, 4> _texelFetch_internal(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex,
    vec<T, 3> texel,
    uint lod,
    vec<T, 3> offset = vec<T, 3>(0))
{
  return tex.texture->read(tex_coord_3d_to_2d(tex, uint3(texel + offset)), lod);
}

/* imageStore. */

template<typename S, typename T, access A>
inline void _texture_write_internal(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, T _coord, vec<S, 4> value)
{
  float w = tex.texture_size.x;
  float h = tex.texture_size.y;
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h) {
    tex.texture->write(value, uint2(_coord.xy));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(
    thread _mtl_combined_image_sampler_2d_atomic_fallback<S, A> tex, T _coord, vec<S, 4> value)
{
  tex.texture->write(value, uint2(_coord.xy));
}

template<typename S, typename T, access A>
inline void _texture_write_internal(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
    T _coord,
    vec<S, 4> value)
{
  float w = tex.texture_size.x;
  float h = tex.texture_size.y;
  float d = tex.texture_size.z;
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h && _coord.z >= 0 &&
      _coord.z < d)
  {
    tex.texture->write(value, tex_coord_3d_to_2d(tex, uint3(_coord)));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(
    thread _mtl_combined_image_sampler_2d_array_atomic_fallback<S, A> tex,
    T _coord,
    vec<S, 4> value)
{
  tex.texture->write(value, tex_coord_3d_to_2d(tex, uint3(_coord)));
}

template<typename S, typename T, access A>
inline void _texture_write_internal(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, T _coord, vec<S, 4> value)
{
  float w = tex.texture_size.x;
  float h = tex.texture_size.y;
  float d = tex.texture_size.z;
  if (_coord.x >= 0 && _coord.x < w && _coord.y >= 0 && _coord.y < h && _coord.z >= 0 &&
      _coord.z < d)
  {
    tex.texture->write(value, tex_coord_3d_to_2d(tex, uint3(_coord)));
  }
}

template<typename S, typename T, access A>
inline void _texture_write_internal_fast(
    thread _mtl_combined_image_sampler_3d_atomic_fallback<S, A> tex, T _coord, vec<S, 4> value)
{
  tex.texture->write(value, tex_coord_3d_to_2d(tex, uint3(_coord)));
}

#endif

/* Matrix compare operators. */
/** TODO(fclem): Template. */
inline bool operator==(float4x4 a, float4x4 b)
{
  for (int i = 0; i < 4; i++) {
    if (any(a[i] != b[i])) {
      return false;
    }
  }
  return true;
}
inline bool operator==(float3x3 a, float3x3 b)
{
  for (int i = 0; i < 3; i++) {
    if (any(a[i] != b[i])) {
      return false;
    }
  }
  return true;
}
inline bool operator==(float2x2 a, float2x2 b)
{
  for (int i = 0; i < 2; i++) {
    if (any(a[i] != b[i])) {
      return false;
    }
  }
  return true;
}

inline bool operator!=(float4x4 a, float4x4 b)
{
  return !(a == b);
}
inline bool operator!=(float3x3 a, float3x3 b)
{
  return !(a == b);
}
inline bool operator!=(float2x2 a, float2x2 b)
{
  return !(a == b);
}

/* Matrix unary minus operator. */

inline float4x4 operator-(float4x4 a)
{
  float4x4 b;
  for (int i = 0; i < 4; i++) {
    b[i] = -a[i];
  }
  return b;
}
inline float3x3 operator-(float3x3 a)
{
  float3x3 b;
  for (int i = 0; i < 3; i++) {
    b[i] = -a[i];
  }
  return b;
}
inline float2x2 operator-(float2x2 a)
{
  float2x2 b;
  for (int i = 0; i < 2; i++) {
    b[i] = -a[i];
  }
  return b;
}

/* SSBO Vertex Fetch Mode. */
#ifdef MTL_SSBO_VERTEX_FETCH
/**
 * Enabled when geometry is passed via raw buffer bindings, rather than using
 * vertex assembly in the vertex-descriptor.
 *
 *  To describe the layout of input attribute data, we will generate uniforms (defaulting to 0)
 * with the names per unique input attribute with name `attr`:
 *
 * - uniform_ssbo_stride_##attr  -- Representing the stride between element.
 * - uniform_ssbo_offset_##attr  -- Representing the base offset within the vertex.
 * - uniform_ssbo_fetchmode_##attr - Whether using per-vertex (=0) or per-instance fetch (=1).
 * - uniform_ssbo_vbo_id_##attr - buffer binding index for VBO with data for this attribute.
 * - uniform_ssbo_type_##attr - The type of data in the currently bound buffer.
 *
 * If the uniform_ssbo_type_* does not match with the desired type, then it is the responsibility
 * of the shader to perform the conversion. Types should always be read as the raw attribute type,
 * and then converted. e.g. If the uniform_ssbo_type_* is `int`, but we want to read it to be
 * normalized to a float.
 * The implementation should query the attribute type using vertex_fetch_get_attr_type(attr_name):
 *
 * \code{.cc}
 * float fweight = 0.0;
 * if(vertex_fetch_get_attr_type(in_weight) == GPU_SHADER_ATTR_TYPE_INT) {
 *   int iweight = vertex_fetch_attribute(gl_VertexID, in_weight, int);
 *   fweight = (float)iweight/(float)INT32_MAX;
 * } else {
 *   fweight = = vertex_fetch_attribute(gl_VertexID, in_weight, float);
 * }
 * \endcode
 *
 * Note: These uniforms are generated as part of the same data block used for regular uniforms
 * and attribute data is written prior to each draw call, depending on the configuration of
 * the vertex descriptor for an MTLBatch or MTLImmedaite call. */
#  define PPCAT_NX(A, B) A##B
#  define PPCAT(A, B) PPCAT_NX(A, B)

#  define RESOLVE_VERTEX(v_id) \
    ((UNIFORM_SSBO_USES_INDEXED_RENDERING_STR > 0) ? \
         ((UNIFORM_SSBO_INDEX_MODE_U16_STR > 0) ? MTL_INDEX_DATA_U16[v_id] : \
                                                  MTL_INDEX_DATA_U32[v_id]) : \
         v_id)
#  define ATTR_TYPE(attr) PPCAT(SSBO_ATTR_TYPE_, attr)
#  define vertex_fetch_attribute_raw(n, attr, type) \
    (reinterpret_cast<constant type *>( \
        &MTL_VERTEX_DATA[PPCAT(UNIFORM_SSBO_VBO_ID_STR, attr)] \
                        [(PPCAT(UNIFORM_SSBO_STRIDE_STR, attr) * \
                          ((PPCAT(UNIFORM_SSBO_FETCHMODE_STR, attr)) ? gl_InstanceID : n)) + \
                         PPCAT(UNIFORM_SSBO_OFFSET_STR, attr)]))[0]
#  define vertex_fetch_attribute(n, attr, type) \
    vertex_fetch_attribute_raw(RESOLVE_VERTEX(n), attr, type)
#  define vertex_id_from_index_id(n) RESOLVE_VERTEX(n)
#  define vertex_fetch_get_input_prim_type() UNIFORM_SSBO_INPUT_PRIM_TYPE_STR
#  define vertex_fetch_get_input_vert_count() UNIFORM_SSBO_INPUT_VERT_COUNT_STR
#  define vertex_fetch_get_attr_type(attr) PPCAT(UNIFORM_SSBO_TYPE_STR, attr)

/* Must mirror GPU_primitive.hh. */
#  define GPU_PRIM_POINTS 0
#  define GPU_PRIM_LINES 1
#  define GPU_PRIM_TRIS 2
#  define GPU_PRIM_LINE_STRIP 3
#  define GPU_PRIM_LINE_LOOP 4
#  define GPU_PRIM_TRI_STRIP 5
#  define GPU_PRIM_TRI_FAN 6
#  define GPU_PRIM_LINES_ADJ 7
#  define GPU_PRIM_TRIS_ADJ 8
#  define GPU_PRIM_LINE_STRIP_ADJ 9
#endif

/* Common Functions. */
#define dFdx(x) dfdx(x)
#define dFdy(x) dfdy(x)
#define mod(x, y) _mtlmod(x, y)
#define discard discard_fragment()
#define inversesqrt rsqrt

inline float radians(float deg)
{
  /* Constant factor: M_PI_F/180.0. */
  return deg * 0.01745329251f;
}

inline float degrees(float rad)
{
  /* Constant factor: 180.0/M_PI_F. */
  return rad * 57.2957795131;
}

#define select(A, B, C) mix(A, B, C)

/* Type conversions and type truncation. */
inline float4 to_float4(float3 val)
{
  return float4(val, 1.0);
}

/* Type conversions and type truncation (Utility Functions). */
inline float3x3 mat4_to_mat3(float4x4 matrix)
{
  return float3x3(matrix[0].xyz, matrix[1].xyz, matrix[2].xyz);
}

inline int floatBitsToInt(float f)
{
  return as_type<int>(f);
}

inline int2 floatBitsToInt(float2 f)
{
  return as_type<int2>(f);
}

inline int3 floatBitsToInt(float3 f)
{
  return as_type<int3>(f);
}

inline int4 floatBitsToInt(float4 f)
{
  return as_type<int4>(f);
}

inline uint floatBitsToUint(float f)
{
  return as_type<uint>(f);
}

inline uint2 floatBitsToUint(float2 f)
{
  return as_type<uint2>(f);
}

inline uint3 floatBitsToUint(float3 f)
{
  return as_type<uint3>(f);
}

inline uint4 floatBitsToUint(float4 f)
{
  return as_type<uint4>(f);
}

inline float intBitsToFloat(int f)
{
  return as_type<float>(f);
}

inline float2 intBitsToFloat(int2 f)
{
  return as_type<float2>(f);
}

inline float3 intBitsToFloat(int3 f)
{
  return as_type<float3>(f);
}

inline float4 intBitsToFloat(int4 f)
{
  return as_type<float4>(f);
}

inline float uintBitsToFloat(uint f)
{
  return as_type<float>(f);
}

inline float2 uintBitsToFloat(uint2 f)
{
  return as_type<float2>(f);
}

inline float3 uintBitsToFloat(uint3 f)
{
  return as_type<float3>(f);
}

inline float4 uintBitsToFloat(uint4 f)
{
  return as_type<float4>(f);
}

#define bitfieldReverse reverse_bits
#define bitfieldExtract extract_bits
#define bitfieldInsert insert_bits

/* popcount returns the same type as T. bitCount always returns int. */
template<typename T> int bitCount(T x)
{
  return int(popcount(x));
}
template<typename T, int n> vec<int, n> bitCount(vec<T, n> x)
{
  return vec<int, n>(popcount(x));
}

template<typename T> int findLSB(T x)
{
  /* ctz returns the number of trailing zeroes. To fetch the index of the LSB, we can also use this
   * value as index, however we need to filter out the case where the input value is zero to match
   * GLSL functionality. */
  return (x == T(0)) ? int(-1) : int(ctz(x));
}

template<typename T> int findMSB(T x)
{
  /* clz returns the number of leading zeroes. To fetch the index of the MSB, we can also use this
   * value as index when offset by 1. */
  return int(sizeof(T) * 8) - 1 - int(clz(x));
}

/* Texture size functions. Add texture types as needed. */
#define imageSize(image) textureSize(image, 0)

template<typename T, access A>
int textureSize(thread _mtl_combined_image_sampler_1d<T, A> image, uint lod)
{
  return int(image.texture->get_width());
}

template<typename T, access A>
int2 textureSize(thread _mtl_combined_image_sampler_1d_array<T, A> image, uint lod)
{
  return int2(image.texture->get_width(), image.texture->get_array_size());
}

template<typename T, access A>
int2 textureSize(thread _mtl_combined_image_sampler_2d<T, A> image, uint lod)
{
  return int2(image.texture->get_width(lod), image.texture->get_height(lod));
}

template<typename T, access A>
int2 textureSize(thread _mtl_combined_image_sampler_depth_2d<T, A> image, uint lod)
{
  return int2(image.texture->get_width(lod), image.texture->get_height(lod));
}

template<typename T, access A>
int3 textureSize(thread _mtl_combined_image_sampler_2d_array<T, A> image, uint lod)
{
  return int3(image.texture->get_width(lod),
              image.texture->get_height(lod),
              image.texture->get_array_size());
}

template<typename T, access A>
int3 textureSize(thread _mtl_combined_image_sampler_depth_2d_array<T, A> image, uint lod)
{
  return int3(image.texture->get_width(lod),
              image.texture->get_height(lod),
              image.texture->get_array_size());
}

template<typename T, access A>
int2 textureSize(thread _mtl_combined_image_sampler_cube<T, A> image, uint lod)
{
  return int2(image.texture->get_width(lod), image.texture->get_height(lod));
}

template<typename T, access A>
int3 textureSize(thread _mtl_combined_image_sampler_3d<T, A> image, uint lod)
{
  return int3(image.texture->get_width(lod),
              image.texture->get_height(lod),
              image.texture->get_depth(lod));
}

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
/* textureSize functions for fallback atomic textures. */
template<typename T, access A>
int2 textureSize(thread _mtl_combined_image_sampler_2d_atomic_fallback<T, A> image, uint lod)
{
  return int2(image.texture->get_width(lod), image.texture->get_height(lod));
}

template<typename T, access A>
int3 textureSize(thread _mtl_combined_image_sampler_2d_array_atomic_fallback<T, A> image, uint lod)
{
  return int3(image.texture_size);
}

template<typename T, access A>
int3 textureSize(thread _mtl_combined_image_sampler_3d_atomic_fallback<T, A> image, uint lod)
{
  return int3(image.texture_size);
}
#endif

/* Equality and comparison functions. */
#define lessThan(a, b) ((a) < (b))
#define lessThanEqual(a, b) ((a) <= (b))
#define greaterThan(a, b) ((a) > (b))
#define greaterThanEqual(a, b) ((a) >= (b))
#define equal(a, b) ((a) == (b))
#define notEqual(a, b) ((a) != (b))

template<typename T, int n> bool all(vec<T, n> x)
{
  bool _all = true;
  for (int i = 0; i < n; i++) {
    _all = _all && (x[i] > 0);
  }
  return _all;
}

template<typename T, int n> bool any(vec<T, n> x)
{
  bool _any = false;
  for (int i = 0; i < n; i++) {
    _any = _any || (x[i] > 0);
  }
  return _any;
}

/* Modulo functionality. */
int _mtlmod(int a, int b)
{
  return a - b * (a / b);
}

float _mtlmod(float a, float b)
{
  return a - b * floor(a / b);
}

template<typename T, int n> vec<T, n> _mtlmod(vec<T, n> x, vec<T, n> y)
{
  return x - (y * floor(x / y));
}

template<typename T, int n, typename U> vec<T, n> _mtlmod(vec<T, n> x, U y)
{
  return x - (vec<T, n>(y) * floor(x / vec<T, n>(y)));
}

template<typename T, typename U, int n> vec<U, n> _mtlmod(T x, vec<U, n> y)
{
  return vec<U, n>(x) - (y * floor(vec<U, n>(x) / y));
}

/* Mathematical functions. */
template<typename T> T atan(T y, T x)
{
  return atan2(y, x);
}

/* Matrix Inverse. */
float4x4 inverse(float4x4 a)
{
  float b00 = a[0][0] * a[1][1] - a[0][1] * a[1][0];
  float b01 = a[0][0] * a[1][2] - a[0][2] * a[1][0];
  float b02 = a[0][0] * a[1][3] - a[0][3] * a[1][0];
  float b03 = a[0][1] * a[1][2] - a[0][2] * a[1][1];
  float b04 = a[0][1] * a[1][3] - a[0][3] * a[1][1];
  float b05 = a[0][2] * a[1][3] - a[0][3] * a[1][2];
  float b06 = a[2][0] * a[3][1] - a[2][1] * a[3][0];
  float b07 = a[2][0] * a[3][2] - a[2][2] * a[3][0];
  float b08 = a[2][0] * a[3][3] - a[2][3] * a[3][0];
  float b09 = a[2][1] * a[3][2] - a[2][2] * a[3][1];
  float b10 = a[2][1] * a[3][3] - a[2][3] * a[3][1];
  float b11 = a[2][2] * a[3][3] - a[2][3] * a[3][2];

  float inv_det = 1.0 / (b00 * b11 - b01 * b10 + b02 * b09 + b03 * b08 - b04 * b07 + b05 * b06);

  float4x4 adjoint{};
  adjoint[0][0] = a[1][1] * b11 - a[1][2] * b10 + a[1][3] * b09;
  adjoint[0][1] = a[0][2] * b10 - a[0][1] * b11 - a[0][3] * b09;
  adjoint[0][2] = a[3][1] * b05 - a[3][2] * b04 + a[3][3] * b03;
  adjoint[0][3] = a[2][2] * b04 - a[2][1] * b05 - a[2][3] * b03;
  adjoint[1][0] = a[1][2] * b08 - a[1][0] * b11 - a[1][3] * b07;
  adjoint[1][1] = a[0][0] * b11 - a[0][2] * b08 + a[0][3] * b07;
  adjoint[1][2] = a[3][2] * b02 - a[3][0] * b05 - a[3][3] * b01;
  adjoint[1][3] = a[2][0] * b05 - a[2][2] * b02 + a[2][3] * b01;
  adjoint[2][0] = a[1][0] * b10 - a[1][1] * b08 + a[1][3] * b06;
  adjoint[2][1] = a[0][1] * b08 - a[0][0] * b10 - a[0][3] * b06;
  adjoint[2][2] = a[3][0] * b04 - a[3][1] * b02 + a[3][3] * b00;
  adjoint[2][3] = a[2][1] * b02 - a[2][0] * b04 - a[2][3] * b00;
  adjoint[3][0] = a[1][1] * b07 - a[1][0] * b09 - a[1][2] * b06;
  adjoint[3][1] = a[0][0] * b09 - a[0][1] * b07 + a[0][2] * b06;
  adjoint[3][2] = a[3][1] * b01 - a[3][0] * b03 - a[3][2] * b00;
  adjoint[3][3] = a[2][0] * b03 - a[2][1] * b01 + a[2][2] * b00;
  return adjoint * inv_det;
}

float3x3 inverse(float3x3 m)
{
  float b00 = m[1][1] * m[2][2] - m[2][1] * m[1][2];
  float b01 = m[0][1] * m[2][2] - m[2][1] * m[0][2];
  float b02 = m[0][1] * m[1][2] - m[1][1] * m[0][2];

  float inv_det = 1.0 / (m[0][0] * b00 - m[1][0] * b01 + m[2][0] * b02);

  float3x3 adjoint{};
  adjoint[0][0] = +b00;
  adjoint[0][1] = -b01;
  adjoint[0][2] = +b02;
  adjoint[1][0] = -(m[1][0] * m[2][2] - m[2][0] * m[1][2]);
  adjoint[1][1] = +(m[0][0] * m[2][2] - m[2][0] * m[0][2]);
  adjoint[1][2] = -(m[0][0] * m[1][2] - m[1][0] * m[0][2]);
  adjoint[2][0] = +(m[1][0] * m[2][1] - m[2][0] * m[1][1]);
  adjoint[2][1] = -(m[0][0] * m[2][1] - m[2][0] * m[0][1]);
  adjoint[2][2] = +(m[0][0] * m[1][1] - m[1][0] * m[0][1]);
  return adjoint * inv_det;
}

float2x2 inverse(float2x2 m)
{
  float inv_det = 1.0 / (m[0][0] * m[1][1] - m[1][0] * m[0][1]);

  float2x2 adjoint{};
  adjoint[0][0] = +m[1][1];
  adjoint[1][0] = -m[1][0];
  adjoint[0][1] = -m[0][1];
  adjoint[1][1] = +m[0][0];
  return adjoint * inv_det;
}

/* Additional overloads for builtin functions. */
float distance(float x, float y)
{
  return abs(y - x);
}

/* Overload for mix(A, B, float ratio). */
template<typename T, int Size> vec<T, Size> mix(vec<T, Size> a, vec<T, Size> b, float val)
{
  return mix(a, b, vec<T, Size>(val));
}

/* Overload for mix(A, B, bvec<N>). */
template<typename T, int Size>
vec<T, Size> mix(vec<T, Size> a, vec<T, Size> b, vec<int, Size> mask)
{
  vec<T, Size> result;
  for (int i = 0; i < Size; i++) {
    result[i] = mask[i] ? b[i] : a[i];
  }
  return result;
}

/* Using vec<bool, S> does not appear to work, splitting cases. */
/* Overload for mix(A, B, bvec<N>). */
template<typename T> vec<T, 4> mix(vec<T, 4> a, vec<T, 4> b, bvec4 mask)
{
  vec<T, 4> result;
  for (int i = 0; i < 4; i++) {
    result[i] = mask[i] ? b[i] : a[i];
  }
  return result;
}

/* Overload for mix(A, B, bvec<N>). */
template<typename T> vec<T, 3> mix(vec<T, 3> a, vec<T, 3> b, bvec3 mask)
{
  vec<T, 3> result;
  for (int i = 0; i < 3; i++) {
    result[i] = mask[i] ? b[i] : a[i];
  }
  return result;
}

/* Overload for mix(A, B, bvec<N>). */
template<typename T> vec<T, 2> mix(vec<T, 2> a, vec<T, 2> b, bvec2 mask)
{
  vec<T, 2> result;
  for (int i = 0; i < 2; i++) {
    result[i] = mask[i] ? b[i] : a[i];
  }
  return result;
}

/* Overload for mix(A, B, bvec<N>). */
template<typename T> T mix(T a, T b, bool mask)
{
  return (mask) ? b : a;
}

template<typename T, int Size> bool is_zero(vec<T, Size> a)
{
  for (int i = 0; i < Size; i++) {
    if (a[i] != T(0)) {
      return false;
    }
  }
  return true;
}

/**
 * Matrix conversion fallback for functional style casting & constructors.
 * To avoid name collision with the types, they are replaced with uppercase version
 * before compilation.
 */

mat2 MAT2x2(vec2 a, vec2 b)
{
  return mat2(a, b);
}
mat2 MAT2x2(float a1, float a2, float b1, float b2)
{
  return mat2(vec2(a1, a2), vec2(b1, b2));
}
mat2 MAT2x2(float f)
{
  return mat2(f);
}
mat2 MAT2x2(mat3 m)
{
  return mat2(m[0].xy, m[1].xy);
}
mat2 MAT2x2(mat4 m)
{
  return mat2(m[0].xy, m[1].xy);
}

mat3 MAT3x3(vec3 a, vec3 b, vec3 c)
{
  return mat3(a, b, c);
}
mat3 MAT3x3(
    float a1, float a2, float a3, float b1, float b2, float b3, float c1, float c2, float c3)
{
  return mat3(vec3(a1, a2, a3), vec3(b1, b2, b3), vec3(c1, c2, c3));
}
mat3 MAT3x3(vec3 a, float b1, float b2, float b3, float c1, float c2, float c3)
{
  return mat3(a, vec3(b1, b2, b3), vec3(c1, c2, c3));
}
mat3 MAT3x3(float f)
{
  return mat3(f);
}
mat3 MAT3x3(mat4 m)
{
  return mat3(m[0].xyz, m[1].xyz, m[2].xyz);
}
mat3 MAT3x3(mat3x4 m)
{
  return mat3(m[0].xyz, m[1].xyz, m[2].xyz);
}
mat3 MAT3x3(mat2 m)
{
  return mat3(vec3(m[0].xy, 0.0), vec3(m[1].xy, 0.0), vec3(0.0, 0.0, 1.0));
}

mat4 MAT4x4(vec4 a, vec4 b, vec4 c, vec4 d)
{
  return mat4(a, b, c, d);
}
mat4 MAT4x4(float a1,
            float a2,
            float a3,
            float a4,
            float b1,
            float b2,
            float b3,
            float b4,
            float c1,
            float c2,
            float c3,
            float c4,
            float d1,
            float d2,
            float d3,
            float d4)
{
  return mat4(
      vec4(a1, a2, a3, a4), vec4(b1, b2, b3, b4), vec4(c1, c2, c3, c4), vec4(d1, d2, d3, d4));
}
mat4 MAT4x4(float f)
{
  return mat4(f);
}
mat4 MAT4x4(mat3 m)
{
  return mat4(
      vec4(m[0].xyz, 0.0), vec4(m[1].xyz, 0.0), vec4(m[2].xyz, 0.0), vec4(0.0, 0.0, 0.0, 1.0));
}
mat4 MAT4x4(mat3x4 m)
{
  return mat4(m[0], m[1], m[2], vec4(0.0, 0.0, 0.0, 1.0));
}
mat4 MAT4x4(mat4x3 m)
{
  return mat4(m[0][0],
              m[0][1],
              m[0][2],
              0.0,
              m[1][0],
              m[1][1],
              m[1][2],
              0.0,
              m[2][0],
              m[2][1],
              m[2][2],
              0.0,
              m[3][0],
              m[3][1],
              m[3][2],
              0.0);
}
mat4 MAT4x4(mat2 m)
{
  return mat4(vec4(m[0].xy, 0.0, 0.0),
              vec4(m[1].xy, 0.0, 0.0),
              vec4(0.0, 0.0, 1.0, 0.0),
              vec4(0.0, 0.0, 0.0, 1.0));
}

mat3x4 MAT3x4(vec4 a, vec4 b, vec4 c)
{
  return mat3x4(a, b, c);
}
mat3x4 MAT3x4(float a1,
              float a2,
              float a3,
              float a4,
              float b1,
              float b2,
              float b3,
              float b4,
              float c1,
              float c2,
              float c3,
              float c4)
{
  return mat3x4(vec4(a1, a2, a3, a4), vec4(b1, b2, b3, b4), vec4(c1, c2, c3, c4));
}
mat3x4 MAT3x4(float f)
{
  return mat3x4(f);
}
mat3x4 MAT3x4(mat3 m)
{
  return mat3x4(vec4(m[0].xyz, 0.0), vec4(m[1].xyz, 0.0), vec4(m[2].xyz, 0.0));
}
mat3x4 MAT3x4(mat2 m)
{
  return mat3x4(vec4(m[0].xy, 0.0, 0.0), vec4(m[1].xy, 0.0, 0.0), vec4(0.0, 0.0, 1.0, 0.0));
}

#define MAT2 MAT2x2
#define MAT3 MAT3x3
#define MAT4 MAT4x4
