/* SPDX-FileCopyrightText: 2022 Blender Authors
 *
 * SPDX-License-Identifier: GPL-2.0-or-later */

/** Special header for mapping commonly defined tokens to API-specific variations.
 * Where possible, this will adhere closely to base GLSL, where semantics are the same.
 * However, host code shader code may need modifying to support types where necessary variations
 * exist between APIs but are not expressed through the source. (e.g. distinction between depth2d
 * and texture2d types in metal).
 */

/* Suppress unhelpful shader compiler warnings. */
#pragma clang diagnostic ignored "-Wunused-variable"
#pragma clang diagnostic ignored "-Wcomment"

#define ENABLE_IF(cond) thread metal::enable_if_t<(cond)> * = nullptr

/* Base instance with offsets. */
#define gpu_BaseInstance gl_BaseInstanceARB
#define gpu_InstanceIndex (gl_InstanceID + gpu_BaseInstance)

#ifdef MTL_WORKGROUP_SIZE_X
/* Older Metal compiler version don't treat vector component access as constexpr.
 * We have to make a wrapper class for that otherwise we cannot use WorkGroupSize for sizing
 * threadgroup arrays. Note that this bug is not present in the version 4.1 of the compiler. */
struct mtl_WorkGroupSize {
  union {
    struct {
      uint x, y, z;
    };
    uint2 xy;
    uint3 xyz;
  };

  constexpr mtl_WorkGroupSize()
      : x(MTL_WORKGROUP_SIZE_X), y(MTL_WORKGROUP_SIZE_Y), z(MTL_WORKGROUP_SIZE_Z)
  {
  }

  constexpr inline operator uint3() const
  {
    return xyz;
  }
};
#  define gl_WorkGroupSize mtl_WorkGroupSize()
#endif

/* Type definitions. */
/* int implicitly cast to bool in MSL. */
using bool32_t = int32_t;
using vec3_1010102_Unorm = uint32_t;
using vec3_1010102_Inorm = int32_t;
/* GLSL types aliases. */
using vec2 = float2;
using vec3 = float3;
using vec4 = float4;
using mat2x2 = float2x2;
using mat2x3 = float2x3;
using mat2x4 = float2x4;
using mat3x2 = float3x2;
using mat3x3 = float3x3;
using mat3x4 = float3x4;
using mat4x2 = float4x2;
using mat4x3 = float4x3;
using mat4x4 = float4x4;
using mat2 = float2x2;
using mat3 = float3x3;
using mat4 = float4x4;
using ivec2 = int2;
using ivec3 = int3;
using ivec4 = int4;
using uvec2 = uint2;
using uvec3 = uint3;
using uvec4 = uint4;
using bvec2 = bool2;
using bvec3 = bool3;
using bvec4 = bool4;

/* Compute decorators. */
#define barrier() \
  threadgroup_barrier(mem_flags::mem_threadgroup | mem_flags::mem_device | mem_flags::mem_texture)

/** Shader atomics:
 * In order to emulate GLSL-style atomic operations, wherein variables can be used within atomic
 * operations, even if they are not explicitly declared atomic, we can cast the pointer to atomic,
 * to ensure that the load instruction follows atomic_load/store idioms.
 *
 * NOTE: We cannot hoist the address space into the template declaration, so these must be declared
 * for each relevant address space. */
#define ATOMIC_OP_EX(qualifier, glsl_op, mtl_op) \
  template<typename T> T atomic##glsl_op(qualifier T &mem, T data) \
  { \
    return atomic_##mtl_op##_explicit((qualifier _atomic<T> *)&mem, data, memory_order_relaxed); \
  }

#define ATOMIC_OP(glsl_op, mtl_op) \
  ATOMIC_OP_EX(threadgroup, glsl_op, mtl_op) \
  ATOMIC_OP_EX(device, glsl_op, mtl_op)

ATOMIC_OP(Max, fetch_max)
ATOMIC_OP(Min, fetch_min)
ATOMIC_OP(Add, fetch_add)
ATOMIC_OP(Sub, fetch_sub)
ATOMIC_OP(And, fetch_and)
ATOMIC_OP(Or, fetch_or)
ATOMIC_OP(Xor, fetch_xor)
ATOMIC_OP(Exchange, exchange)

#undef ATOMIC_OP

/* Generate wrapper structs for combined texture and sampler type. */
#ifdef USE_ARGUMENT_BUFFER_FOR_SAMPLERS
using sampler_ptr = constant sampler *;
#else
using sampler_ptr = thread sampler *;
#endif

/* Use point sampler instead of texture read to benefit from texture caching and reduce branching
 * through removal of bounds tests, as these are handled by the sample operation. */
constexpr sampler _mtl_fetch_samp(address::clamp_to_zero, filter::nearest, coord::pixel);

template<typename T,
         access A,
         typename TextureT,
         bool is_depth,
         int Dim,
         int Cube,
         int Array,
         bool Atomic = false>
struct _mtl_sampler {

  template<typename U, int S>
  using vec_or_scalar = typename metal::conditional<S == 1, U, vec<U, S>>::type;

  using FltCoord = vec_or_scalar<float, Dim + Cube + Array>;
  using FltDeriv = vec_or_scalar<float, Dim + Cube>;
  using IntCoord = vec_or_scalar<int, Dim + Cube + Array>;
  using IntDeriv = vec_or_scalar<int, Dim + Cube>;
  using UintCoord = vec_or_scalar<uint, Dim + Cube + Array>;
  using UintDeriv = vec_or_scalar<uint, Dim + Cube>;
  using SizeVec = vec_or_scalar<int, Dim + Array>;
  using DataVec = vec<T, 4>;
  using AtomicT = T;

  /* Template compatible gradient type choosing. */
  template<int D, int C> struct gradient_n {};
  /* clang-format off */
  template<> struct gradient_n<2, 0> { using type = gradient2d; };
  template<> struct gradient_n<3, 0> { using type = gradient3d; };
  template<> struct gradient_n<2, 1> { using type = gradientcube; };
  /* clang-format on */
  /* Using `using` would invalidate the whole class. */
#define gradient typename gradient_n<Dim, Cube>::type

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
  /* If native texture atomics are unsupported, we instead utilize a custom type which wraps a
   * buffer-backed texture. This texture will always be a Texture2D, but will emulate access to
   * Texture3D and Texture2DArray by stacking layers.
   * Access pattern will be derived based on the source type. 2DArray and 3D atomic texture
   * support will require information on the size of each layer within the source 2D texture.
   *
   * A device pointer to the backing buffer will also be available for the atomic operations.
   * NOTE: For atomic ops, it will only be valid to use access::read_write.
   * We still need to use the wrapped type for access:sample, as texture2DArray and texture3D
   * will require access indirection.
   *
   * NOTE: Only type of UINT is valid, but full template provided to match syntax of standard
   * textures. */
  template<typename U, int D, bool At> struct AtomicEmulation {};

  template<typename U> struct AtomicEmulation<U, 2, true> {
    /** Buffer to do atomic operations on. */
    device U *buffer;
    /* Aligned width matches the number of buffer elements in bytes_per_row. This may be greater
     * than the texture's native width to satisfy device alignment rules. We need to use the padded
     * width when writing to ensure the correct writing location aligns with a given pixel location
     * in the texture. */
    ushort aligned_width;

    /* Convert 2D coordinates to the backing 2D texture coordinates. */
    int2 to_internal_coord(thread TextureT * /*texture*/, IntCoord coord) const
    {
      return coord.xy;
    }
    /* Convert 2D coordinates to the memory location in the 2D texture memory buffer. */
    uint to_linear_coord(thread TextureT * /*texture*/, IntCoord coord) const
    {
      return coord.x + coord.y * aligned_width;
    }
  };

  template<typename U> struct AtomicEmulation<U, 3, true> {
    /** Buffer to do atomic operations on. */
    device U *buffer;
    /**
     * Required for pixel location inside the backing texture 2D space, and texture size query.
     *
     * IMPORTANT: Note that the slices are not contiguous in the backing texture as we simply use
     * it for linear storage.
     * For instance, for a [2,2,2] texture store in a [6x4] backing texture:
     *
     * \code{.unparsed}
     * 001122
     * 334455
     * 667788
     * xxxxxx
     * \endcode
     *
     * The numbers are rows in the 3D texture.
     */
    ushort3 texture_size;
    /* Aligned width matches the number of buffer elements in bytes_per_row. This may be greater
     * than the texture's native width to satisfy device alignment rules. We need to use the padded
     * width when writing to ensure the correct writing location aligns with a given pixel location
     * in the texture. */
    ushort aligned_width;

    /* Convert 3D coordinates to the backing 2D texture coordinates. */
    int2 to_internal_coord(thread TextureT *texture, IntCoord coord) const
    {
      /* Index of a pixel in the data array. Assuming data layout is scan-line. */
      uint linear_pixel = uint(coord.x) + uint(coord.y) * texture_size.x +
                          uint(coord.z) * (texture_size.x * texture_size.y);

      uint backing_tex_width = texture->get_width();
      /* Coordinate inside the backing texture. */
      uint y = linear_pixel / backing_tex_width;
      /* Avoid use of modulo operator for speed. */
      uint x = linear_pixel - y * backing_tex_width;
      return int2(x, y);
    }

    /* Convert 3D coordinates to the memory location in the 2D texture memory buffer. */
    uint to_linear_coord(thread TextureT *texture, IntCoord coord) const
    {
      uint2 co = uint2(to_internal_coord(texture, coord));
      return co.x + co.y * uint(aligned_width);
    }
  };

  AtomicEmulation<T, Dim + Array, Atomic> atomic;
#endif

  thread TextureT *texture;
  sampler_ptr samp;

  template<int dim, int array, bool At = false> SizeVec size_impl(uint lod) const {}
  template<> SizeVec size_impl<1, 0>(uint lod) const
  {
    return SizeVec(texture->get_width());
  }
  template<> SizeVec size_impl<1, 1>(uint lod) const
  {
    return SizeVec(texture->get_width(), texture->get_array_size());
  }
  template<> SizeVec size_impl<2, 0>(uint lod) const
  {
    return SizeVec(texture->get_width(lod), texture->get_height(lod));
  }
  template<> SizeVec size_impl<2, 1>(uint lod) const
  {
    return SizeVec(texture->get_width(lod), texture->get_height(lod), texture->get_array_size());
  }
  template<> SizeVec size_impl<3, 0>(uint lod) const
  {
    return SizeVec(texture->get_width(lod), texture->get_height(lod), texture->get_depth(lod));
  }
#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
  template<> SizeVec size_impl<2, 1, true>(uint lod) const
  {
    return SizeVec(atomic.texture_size);
  }
  template<> SizeVec size_impl<3, 0, true>(uint lod) const
  {
    return SizeVec(atomic.texture_size);
  }
#endif
  SizeVec size(int lod = 0) const
  {
    return size_impl<Dim, Array, Atomic>(uint(lod));
  }

#define ARRAY_FN \
  template<int Ar = Array, bool At = Atomic, ENABLE_IF(Ar == 1), ENABLE_IF(At == false)>
#define NON_ARRAY_FN \
  template<int Ar = Array, bool At = Atomic, ENABLE_IF(Ar == 0), ENABLE_IF(At == false)>

  NON_ARRAY_FN DataVec sample(FltCoord coord) const
  {
    return texture->sample(*samp, coord);
  }
  NON_ARRAY_FN DataVec sample_grad(FltCoord coord, FltDeriv dPdx, FltDeriv dPdy) const
  {
    return texture->sample(*samp, coord, gradient(dPdx, dPdy));
  }
  NON_ARRAY_FN DataVec sample_bias(FltCoord coord, bias lod_bias) const
  {
    return texture->sample(*samp, coord, lod_bias);
  }
  NON_ARRAY_FN DataVec sample_lod(FltCoord coord, level lod, IntDeriv offset = {0}) const
  {
    return texture->sample(*samp, coord, lod, offset);
  }
  NON_ARRAY_FN DataVec gather(FltCoord coord) const
  {
    return texture->gather(*samp, coord);
  }
  NON_ARRAY_FN DataVec fetch(IntCoord coord) const
  {
    return texture->sample(_mtl_fetch_samp, FltCoord(coord));
  }
  NON_ARRAY_FN DataVec fetch(IntCoord coord, level lod, IntDeriv offset = {0}) const
  {
    return texture->sample(_mtl_fetch_samp, FltCoord(coord), lod, offset);
  }

  ARRAY_FN DataVec sample(FltCoord coord) const
  {
    return texture->sample(*samp, uv_mask(coord), layer_mask(coord));
  }
  ARRAY_FN DataVec sample_grad(FltCoord coord, FltDeriv dPdx, FltDeriv dPdy) const
  {
    return texture->sample(*samp, uv_mask(coord), layer_mask(coord), gradient(dPdx, dPdy));
  }
  ARRAY_FN DataVec sample_bias(FltCoord coord, bias lod_bias) const
  {
    return texture->sample(*samp, uv_mask(coord), layer_mask(coord), lod_bias);
  }
  ARRAY_FN DataVec sample_lod(FltCoord coord, level lod, IntDeriv offset = {0}) const
  {
    return texture->sample(*samp, uv_mask(coord), layer_mask(coord), lod, offset);
  }
  ARRAY_FN DataVec gather(FltCoord coord) const
  {
    return texture->gather(*samp, uv_mask(coord), layer_mask(coord));
  }
  ARRAY_FN DataVec fetch(IntCoord coord) const
  {
    return texture->sample(_mtl_fetch_samp, uv_mask(coord), layer_mask(coord));
  }
  ARRAY_FN DataVec fetch(IntCoord coord, level lod, IntDeriv ofs = {0}) const
  {
    return texture->sample(_mtl_fetch_samp, uv_mask(coord), layer_mask(coord), lod, ofs);
  }

#undef gradient
#undef ARRAY_FN
#undef NON_ARRAY_FN

  /**
   * Image functions.
   * To be split to its own class.
   */

#define ARRAY_FN \
  template<int Ar = Array, bool At = Atomic, ENABLE_IF(Ar == 1), ENABLE_IF(At == false)>
#define NON_ARRAY_FN \
  template<int Ar = Array, bool At = Atomic, ENABLE_IF(Ar == 0), ENABLE_IF(At == false)>

  NON_ARRAY_FN DataVec load(IntCoord coord) const
  {
    return texture->read(UintCoord(coord), 0);
  }
  NON_ARRAY_FN void store(DataVec data, IntCoord coord) const
  {
    texture->write(data, UintCoord(coord), 0);
  }

  ARRAY_FN DataVec load(IntCoord coord) const
  {
    return texture->read(uv_mask_img(coord), layer_mask_img(coord), 0);
  }
  ARRAY_FN void store(DataVec data, IntCoord coord) const
  {
    texture->write(data, uv_mask_img(coord), layer_mask_img(coord), 0);
  }

#undef ARRAY_FN
#undef NON_ARRAY_FN

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
  /* Atomic samplers only support `texelFetch` as the texture layout doesn't allow filtering. */

#  define ATOMIC_FN template<bool At = Atomic, ENABLE_IF(At == true)>

  ATOMIC_FN DataVec fetch(IntCoord coord) const
  {
    int2 coord_2d = atomic.to_internal_coord(texture, coord);
    return texture->sample(_mtl_fetch_samp, float2(coord_2d));
  }
  ATOMIC_FN DataVec fetch(IntCoord coord, level lod, IntDeriv offset = {0}) const
  {
    int2 coord_2d = atomic.to_internal_coord(texture, coord);
    return texture->sample(_mtl_fetch_samp, float2(coord_2d), lod, offset);
  }

  ATOMIC_FN DataVec load(IntCoord coord) const
  {
    int2 coord_2d = atomic.to_internal_coord(texture, coord);
    return texture->read(uint2(coord_2d), 0);
  }
  ATOMIC_FN void store(DataVec data, IntCoord coord) const
  {
    int2 coord_2d = atomic.to_internal_coord(texture, coord);
    texture->write(data, uint2(coord_2d), 0);
  }

#  undef ATOMIC_FN

  AtomicT atomic_min(IntCoord coord, AtomicT data) const
  {
    return atomicMin(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_max(IntCoord coord, AtomicT data) const
  {
    return atomicMax(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_add(IntCoord coord, AtomicT data) const
  {
    return atomicAdd(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_and(IntCoord coord, AtomicT data) const
  {
    return atomicAnd(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_or(IntCoord coord, AtomicT data) const
  {
    return atomicOr(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_xor(IntCoord coord, AtomicT data) const
  {
    return atomicXor(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }
  AtomicT atomic_exchange(IntCoord coord, AtomicT data) const
  {
    return atomicExchange(atomic.buffer[atomic.to_linear_coord(texture, coord)], data);
  }

#else
#  define NON_ARRAY_ATOMIC \
    template<typename U = T, \
             int Ar = Array, \
             ENABLE_IF(metal::is_integral_v<U> == true), \
             ENABLE_IF(sizeof(U) == 4), \
             ENABLE_IF(Ar == 0)>
#  define ARRAY_ATOMIC \
    template<typename U = T, \
             int Ar = Array, \
             ENABLE_IF(metal::is_integral_v<U> == true), \
             ENABLE_IF(sizeof(U) == 4), \
             ENABLE_IF(Ar == 1)>

  NON_ARRAY_ATOMIC AtomicT atomic_min(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_min(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_max(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_max(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_add(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_add(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_and(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_and(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_or(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_or(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_xor(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_xor(UintCoord(coord), data).x;
  }
  NON_ARRAY_ATOMIC AtomicT atomic_exchange(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_exchange(UintCoord(coord), data).x;
  }

  ARRAY_ATOMIC AtomicT atomic_min(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_min(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_max(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_max(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_add(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_add(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_and(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_and(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_or(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_or(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_xor(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_fetch_xor(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }
  ARRAY_ATOMIC AtomicT atomic_exchange(IntCoord coord, AtomicT data) const
  {
    return texture->atomic_exchange(uv_mask_img(coord), layer_mask_img(coord), data).x;
  }

#  undef NON_ARRAY_ATOMIC
#  undef ARRAY_ATOMIC
#endif

  void fence()
  {
    texture->fence();
  }

 private:
  template<typename U, typename V> static U reshape(V v) {}
  /* clang-format off */
  template<> float reshape<float>(float2 v) { return v.x; }
  template<> float2 reshape<float2>(float3 v) { return v.xy; }
  template<> float3 reshape<float3>(float4 v) { return v.xyz; }
  template<> int reshape<int>(int2 v) { return v.x; }
  template<> int2 reshape<int2>(int3 v) { return v.xy; }
  template<> int3 reshape<int3>(int4 v) { return v.xyz; }
  /* clang-format on */

  FltDeriv uv_mask(FltCoord coord) const
  {
    return reshape<FltDeriv>(coord);
  }
  FltDeriv uv_mask(IntCoord coord) const
  {
    return FltDeriv(reshape<IntDeriv>(coord));
  }

  uint layer_mask(FltCoord coord) const
  {
    return coord[Dim + Cube];
  }
  uint layer_mask(IntCoord coord) const
  {
    return coord[Dim + Cube];
  }

  UintDeriv uv_mask_img(IntCoord coord) const
  {
    return UintDeriv(reshape<IntDeriv>(coord));
  }

  uint layer_mask_img(IntCoord coord) const
  {
    return coord[Dim + Cube];
  }
};

/** Sampler functions */

#define SAMPLER_FN \
  template<typename SamplerT, \
           typename FltCoord = typename SamplerT::FltCoord, \
           typename FltDeriv = typename SamplerT::FltDeriv, \
           typename IntCoord = typename SamplerT::IntCoord, \
           typename IntDeriv = typename SamplerT::IntDeriv, \
           typename UintCoord = typename SamplerT::UintCoord, \
           typename UintDeriv = typename SamplerT::UintDeriv, \
           typename SizeVec = typename SamplerT::SizeVec, \
           typename DataVec = typename SamplerT::DataVec>

SAMPLER_FN SizeVec textureSize(SamplerT texture, int lod)
{
  return texture.size(lod);
}

SAMPLER_FN DataVec texture(SamplerT texture, FltCoord coord)
{
  return texture.sample(coord);
}

SAMPLER_FN DataVec texture(SamplerT texture, FltCoord coord, float lod_bias)
{
  return texture.sample_bias(coord, bias(lod_bias));
}

SAMPLER_FN DataVec textureLod(SamplerT texture, FltCoord coord, float lod)
{
  return texture.sample_lod(coord, level(lod));
}

SAMPLER_FN DataVec textureLodOffset(SamplerT texture, FltCoord coord, float lod, IntDeriv offset)
{
  return texture.sample_lod(coord, level(lod), offset);
}

SAMPLER_FN DataVec textureGather(SamplerT texture, FltCoord coord)
{
  return texture.gather(coord);
}

SAMPLER_FN DataVec textureGrad(SamplerT texture, FltCoord coord, FltDeriv dPdx, FltDeriv dPdy)
{
  return texture.sample_grad(coord, dPdx, dPdy);
}

SAMPLER_FN DataVec texelFetch(SamplerT texture, IntCoord coord, int lod)
{
  return texture.fetch(coord, level(lod));
}

SAMPLER_FN DataVec texelFetchOffset(SamplerT texture, IntCoord coord, int lod, IntDeriv offset)
{
  return texture.fetch(coord, level(lod), offset);
}

#undef SAMPLER_FN

/** Image functions */

#define IMAGE_FN \
  template<typename SamplerT, \
           typename IntCoord = typename SamplerT::IntCoord, \
           typename IntDeriv = typename SamplerT::IntDeriv, \
           typename UintCoord = typename SamplerT::UintCoord, \
           typename UintDeriv = typename SamplerT::UintDeriv, \
           typename SizeVec = typename SamplerT::SizeVec, \
           typename DataVec = typename SamplerT::DataVec, \
           typename AtomicT = typename SamplerT::AtomicT>

IMAGE_FN SizeVec imageSize(SamplerT texture)
{
  return texture.size();
}

IMAGE_FN void imageFence(SamplerT texture)
{
  return texture.fence();
}

IMAGE_FN DataVec imageLoad(SamplerT texture, IntCoord coord)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return DataVec(0);
  }
  return texture.load(coord);
}

IMAGE_FN DataVec imageLoadFast(SamplerT texture, IntCoord coord)
{
  return texture.load(coord);
}

IMAGE_FN void imageStore(SamplerT texture, IntCoord coord, DataVec data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return;
  }
  texture.store(data, coord);
}

IMAGE_FN
void imageStoreFast(SamplerT texture, IntCoord coord, DataVec data)
{
  texture.store(data, coord);
}

IMAGE_FN AtomicT imageAtomicMin(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_min(coord, data);
}
IMAGE_FN AtomicT imageAtomicMax(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_max(coord, data);
}
IMAGE_FN AtomicT imageAtomicAdd(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_add(coord, data);
}
IMAGE_FN AtomicT imageAtomicAnd(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_and(coord, data);
}
IMAGE_FN AtomicT imageAtomicOr(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_or(coord, data);
}
IMAGE_FN AtomicT imageAtomicXor(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_xor(coord, data);
}
IMAGE_FN AtomicT imageAtomicExchange(SamplerT texture, IntCoord coord, AtomicT data)
{
  if (any(UintCoord(coord) >= UintCoord(texture.size()))) {
    return AtomicT(0);
  }
  return texture.atomic_exchange(coord, data);
}

#undef IMAGE_FN

/* Add any types as needed. */
#define TEMPLATE template<typename T = float, access A = access::sample>
TEMPLATE using depth2D = _mtl_sampler<T, A, depth2d<T, A>, true, 2, 0, 0>;
TEMPLATE using depth2DArray = _mtl_sampler<T, A, depth2d_array<T, A>, true, 2, 0, 1>;
TEMPLATE using depthCube = _mtl_sampler<T, A, texturecube<T, A>, true, 2, 1, 1>;
TEMPLATE using depthCubeArray = _mtl_sampler<T, A, texturecube_array<T, A>, true, 2, 1, 1>;
TEMPLATE using sampler1D = _mtl_sampler<T, A, texture1d<T, A>, false, 1, 0, 0>;
TEMPLATE using sampler1DArray = _mtl_sampler<T, A, texture1d_array<T, A>, false, 1, 0, 1>;
TEMPLATE using sampler2D = _mtl_sampler<T, A, texture2d<T, A>, false, 2, 0, 0>;
TEMPLATE using sampler2DArray = _mtl_sampler<T, A, texture2d_array<T, A>, false, 2, 0, 1>;
TEMPLATE using sampler3D = _mtl_sampler<T, A, texture3d<T, A>, false, 3, 0, 0>;
TEMPLATE using samplerBuffer = _mtl_sampler<T, A, texture_buffer<T, A>, false, 1, 0, 0>;
TEMPLATE using samplerCube = _mtl_sampler<T, A, texturecube<T, A>, false, 2, 1, 0>;
TEMPLATE using samplerCubeArray = _mtl_sampler<T, A, texturecube_array<T, A>, false, 2, 1, 1>;
/* Atomic textures are defined as 2D textures with special layout for 3D texture emulation. */
TEMPLATE using sampler2DAtomic = _mtl_sampler<T, A, texture2d<T, A>, false, 2, 0, 0, true>;
TEMPLATE using sampler2DArrayAtomic = _mtl_sampler<T, A, texture2d<T, A>, false, 2, 0, 1, true>;
TEMPLATE using sampler3DAtomic = _mtl_sampler<T, A, texture2d<T, A>, false, 3, 0, 0, true>;

/* Used by backend to declare the samplers. Could be removed. */
TEMPLATE using _mtl_sampler_depth_2d = depth2D<T, A>;
TEMPLATE using _mtl_sampler_depth_2d_array = depth2DArray<T, A>;
TEMPLATE using _mtl_sampler_depth_cube = depthCube<T, A>;
TEMPLATE using _mtl_sampler_depth_cube_array = depthCubeArray<T, A>;
TEMPLATE using _mtl_sampler_1d = sampler1D<T, A>;
TEMPLATE using _mtl_sampler_1d_array = sampler1DArray<T, A>;
TEMPLATE using _mtl_sampler_2d = sampler2D<T, A>;
TEMPLATE using _mtl_sampler_2d_array = sampler2DArray<T, A>;
TEMPLATE using _mtl_sampler_3d = sampler3D<T, A>;
TEMPLATE using _mtl_sampler_buffer = samplerBuffer<T, A>;
TEMPLATE using _mtl_sampler_cube = samplerCube<T, A>;
TEMPLATE using _mtl_sampler_cube_array = samplerCubeArray<T, A>;
TEMPLATE using _mtl_sampler_2d_atomic = sampler2DAtomic<T, A>;
TEMPLATE using _mtl_sampler_2d_array_atomic = sampler2DArrayAtomic<T, A>;
TEMPLATE using _mtl_sampler_3d_atomic = sampler3DAtomic<T, A>;
#undef TEMPLATE

/* Variant for 1D samplers. Discard the lod. */
template<typename T, access A>
typename sampler1D<T, A>::DataVec texelFetch(sampler1D<T, A> texture, int coord, int lod = 0)
{
  return texture.fetch(coord);
}

/* Variant for 1DArray samplers. Discard the lod. */
template<typename T, access A>
typename sampler1DArray<T, A>::DataVec texelFetch(sampler1DArray<T, A> texture,
                                                  int2 coord,
                                                  int lod = 0)
{
  return texture.fetch(coord);
}

/* Variant for buffer samplers. Discard the lod. */
template<typename T, access A>
typename samplerBuffer<T, A>::DataVec texelFetch(samplerBuffer<T, A> texture,
                                                 int coord,
                                                 int lod = 0)
{
  uint texel = uint(coord);
  if (texel < texture.texture->get_width()) {
    return texture.texture->read(texel);
  }
  return typename samplerBuffer<T, A>::DataVec(0);
}

/* Sampler struct for argument buffer. */
#ifdef USE_ARGUMENT_BUFFER_FOR_SAMPLERS
struct SStruct {
  array<sampler, ARGUMENT_BUFFER_NUM_SAMPLERS> sampler_args [[id(0)]];
};
#endif

/* Samplers as function parameters. */
#define sampler1D thread _mtl_sampler_1d<float>
#define sampler1DArray thread _mtl_sampler_1d_array<float>
#define sampler2D thread _mtl_sampler_2d<float>
#define depth2D thread _mtl_sampler_depth_2d<float>
#define sampler2DArray thread _mtl_sampler_2d_array<float>
#define sampler2DArrayShadow thread _mtl_sampler_depth_2d_array<float>
#define depth2DArray thread _mtl_sampler_depth_2d_array<float>
#define depth2DArrayShadow thread _mtl_sampler_depth_2d_array<float>
#define sampler3D thread _mtl_sampler_3d<float>
#define samplerBuffer thread _mtl_sampler_buffer<float, access::read>
#define samplerCube thread _mtl_sampler_cube<float>
#define samplerCubeArray thread _mtl_sampler_cube_array<float>

#define usampler1D thread _mtl_sampler_1d<uint>
#define usampler1DArray thread _mtl_sampler_1d_array<uint>
#define usampler2D thread _mtl_sampler_2d<uint>
#define udepth2D thread _mtl_sampler_depth_2d<uint>
#define usampler2DArray thread _mtl_sampler_2d_array<uint>
#define usampler2DArrayShadow thread _mtl_sampler_depth_2d_array<uint>
#define udepth2DArrayShadow thread _mtl_sampler_depth_2d_array<uint>
#define usampler3D thread _mtl_sampler_3d<uint>
#define usamplerBuffer thread _mtl_sampler_buffer<uint, access::read>
#define usamplerCube thread _mtl_sampler_cube<uint>
#define usamplerCubeArray thread _mtl_sampler_cube_array<uint>

#define isampler1D thread _mtl_sampler_1d<int>
#define isampler1DArray thread _mtl_sampler_1d_array<int>
#define isampler2D thread _mtl_sampler_2d<int>
#define idepth2D thread _mtl_sampler_depth_2d<int>
#define isampler2DArray thread _mtl_sampler_2d_array<int>
#define isampler2DArrayShadow thread _mtl_sampler_depth_2d_array<int>
#define idepth2DArrayShadow thread _mtl_sampler_depth_2d_array<int>
#define isampler3D thread _mtl_sampler_3d<int>
#define isamplerBuffer thread _mtl_sampler_buffer<int, access::read>
#define isamplerCube thread _mtl_sampler_cube<int>
#define isamplerCubeArray thread _mtl_sampler_cube_array<int>

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
/* If texture atomics are unsupported, map atomic types to internal atomic fallback type. */
#  define usampler2DArrayAtomic _mtl_sampler_2d_array_atomic<uint>
#  define usampler2DAtomic _mtl_sampler_2d_atomic<uint>
#  define usampler3DAtomic _mtl_sampler_3d_atomic<uint>
#  define isampler2DArrayAtomic _mtl_sampler_2d_array_atomic<int>
#  define isampler2DAtomic _mtl_sampler_2d_atomic<int>
#  define isampler3DAtomic _mtl_sampler_3d_atomic<int>
#else
#  define usampler2DArrayAtomic usampler2DArray
#  define usampler2DAtomic usampler2D
#  define usampler3DAtomic usampler3D
#  define isampler2DArrayAtomic isampler2DArray
#  define isampler2DAtomic isampler2D
#  define isampler3DAtomic isampler3D
#endif

/* Vector accessor aliases. */
#define st xy

/* Matrix compare operators. */
#define EQ_OP(type, ...) \
  inline bool operator==(type a, type b) \
  { \
    return __VA_ARGS__; \
  }
EQ_OP(float2x2, all(a[0] == b[0]) && all(a[1] == b[1]))
EQ_OP(float2x3, all(a[0] == b[0]) && all(a[1] == b[1]))
EQ_OP(float2x4, all(a[0] == b[0]) && all(a[1] == b[1]))
EQ_OP(float3x2, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]))
EQ_OP(float3x3, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]))
EQ_OP(float3x4, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]))
EQ_OP(float4x2, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]) && all(a[3] == b[3]))
EQ_OP(float4x3, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]) && all(a[3] == b[3]))
EQ_OP(float4x4, all(a[0] == b[0]) && all(a[1] == b[1]) && all(a[2] == b[2]) && all(a[3] == b[3]))
#undef EQ_OP

template<int N, int M> inline bool operator!=(matrix<float, N, M> a, matrix<float, N, M> b)
{
  return !(a == b);
}

/* Matrix unary minus operator. */
template<int N, int M> inline matrix<float, N, M> operator-(matrix<float, N, M> a)
{
  return a * -1.0;
}

/* Common Functions. */
#define dFdx(x) dfdx(x)
#define dFdy(x) dfdy(x)
#define discard discard_fragment()
#define inversesqrt rsqrt

/* clang-format off */
inline float radians(float deg) { return deg * 0.01745329251; /* M_PI_F / 180 */ }
inline float degrees(float rad) { return rad * 57.2957795131; /* 180 / M_PI_F */ }
inline int floatBitsToInt(float f) { return as_type<int>(f); }
inline int2 floatBitsToInt(float2 f) { return as_type<int2>(f); }
inline int3 floatBitsToInt(float3 f) { return as_type<int3>(f); }
inline int4 floatBitsToInt(float4 f) { return as_type<int4>(f); }
inline uint floatBitsToUint(float f) { return as_type<uint>(f); }
inline uint2 floatBitsToUint(float2 f) { return as_type<uint2>(f); }
inline uint3 floatBitsToUint(float3 f) { return as_type<uint3>(f); }
inline uint4 floatBitsToUint(float4 f) { return as_type<uint4>(f); }
inline float intBitsToFloat(int f) { return as_type<float>(f); }
inline float2 intBitsToFloat(int2 f) { return as_type<float2>(f); }
inline float3 intBitsToFloat(int3 f) { return as_type<float3>(f); }
inline float4 intBitsToFloat(int4 f) { return as_type<float4>(f); }
inline float uintBitsToFloat(uint f) { return as_type<float>(f); }
inline float2 uintBitsToFloat(uint2 f) { return as_type<float2>(f); }
inline float3 uintBitsToFloat(uint3 f) { return as_type<float3>(f); }
inline float4 uintBitsToFloat(uint4 f) { return as_type<float4>(f); }
/* clang-format on */

#define bitfieldReverse reverse_bits
#define bitfieldExtract extract_bits
#define bitfieldInsert insert_bits

/* popcount returns the same type as T. bitCount always returns int. */
template<typename T> int bitCount(T x)
{
  return int(popcount(x));
}
template<typename T, int n> vec<int, n> bitCount(vec<T, n> x)
{
  return vec<int, n>(popcount(x));
}

template<typename T> int findLSB(T x)
{
  /* ctz returns the number of trailing zeroes. To fetch the index of the LSB, we can also use this
   * value as index, however we need to filter out the case where the input value is zero to match
   * GLSL functionality. */
  return (x == T(0)) ? int(-1) : int(ctz(x));
}

template<typename T> int findMSB(T x)
{
  /* clz returns the number of leading zeroes. To fetch the index of the MSB, we can also use this
   * value as index when offset by 1. */
  return int(sizeof(T) * 8) - 1 - int(clz(x));
}

#ifndef MTL_SUPPORTS_TEXTURE_ATOMICS
/* textureSize functions for fallback atomic textures. */
template<typename T, access A>
int2 textureSize(thread _mtl_sampler_2d_atomic<T, A> image, uint lod)
{
  return int2(image.texture->get_width(lod), image.texture->get_height(lod));
}

template<typename T, access A>
int3 textureSize(thread _mtl_sampler_2d_array_atomic<T, A> image, uint lod)
{
  return int3(image.texture_size);
}

template<typename T, access A>
int3 textureSize(thread _mtl_sampler_3d_atomic<T, A> image, uint lod)
{
  return int3(image.texture_size);
}
#endif

#define unpackUnorm4x8 unpack_unorm4x8_to_float
#define unpackSnorm4x8 unpack_snorm4x8_to_float
#define unpackUnorm2x16 unpack_unorm2x16_to_float
#define unpackSnorm2x16 unpack_snorm2x16_to_float

/* Equality and comparison functions. */
#define lessThan(a, b) ((a) < (b))
#define lessThanEqual(a, b) ((a) <= (b))
#define greaterThan(a, b) ((a) > (b))
#define greaterThanEqual(a, b) ((a) >= (b))
#define equal(a, b) ((a) == (b))
#define notEqual(a, b) ((a) != (b))

/* Modulo functionality. */
/* `mod(x, y)` is defined as `x - (y * floor(x / y))` in the metal specification.
 * This is not compatible with GLSL implementation. So we override it with a compatible one. */
#define mod(x, y) _compatible_mod(x, y)
#define MOD \
  { \
    return x - y * floor(x / y); \
  }
float _compatible_mod(float x, float y) MOD;
template<int S> vec<float, S> _compatible_mod(vec<float, S> x, float y) MOD;
template<int S> vec<float, S> _compatible_mod(vec<float, S> x, vec<float, S> y) MOD;
#undef MOD

/* Mathematical functions. */
template<typename T> T atan(T y, T x)
{
  return atan2(y, x);
}

/* Matrix Inverse. */
float4x4 inverse(float4x4 a)
{
  float b00 = a[0][0] * a[1][1] - a[0][1] * a[1][0];
  float b01 = a[0][0] * a[1][2] - a[0][2] * a[1][0];
  float b02 = a[0][0] * a[1][3] - a[0][3] * a[1][0];
  float b03 = a[0][1] * a[1][2] - a[0][2] * a[1][1];
  float b04 = a[0][1] * a[1][3] - a[0][3] * a[1][1];
  float b05 = a[0][2] * a[1][3] - a[0][3] * a[1][2];
  float b06 = a[2][0] * a[3][1] - a[2][1] * a[3][0];
  float b07 = a[2][0] * a[3][2] - a[2][2] * a[3][0];
  float b08 = a[2][0] * a[3][3] - a[2][3] * a[3][0];
  float b09 = a[2][1] * a[3][2] - a[2][2] * a[3][1];
  float b10 = a[2][1] * a[3][3] - a[2][3] * a[3][1];
  float b11 = a[2][2] * a[3][3] - a[2][3] * a[3][2];
  float4x4 adjoint;
  adjoint[0][0] = a[1][1] * b11 - a[1][2] * b10 + a[1][3] * b09;
  adjoint[0][1] = a[0][2] * b10 - a[0][1] * b11 - a[0][3] * b09;
  adjoint[0][2] = a[3][1] * b05 - a[3][2] * b04 + a[3][3] * b03;
  adjoint[0][3] = a[2][2] * b04 - a[2][1] * b05 - a[2][3] * b03;
  adjoint[1][0] = a[1][2] * b08 - a[1][0] * b11 - a[1][3] * b07;
  adjoint[1][1] = a[0][0] * b11 - a[0][2] * b08 + a[0][3] * b07;
  adjoint[1][2] = a[3][2] * b02 - a[3][0] * b05 - a[3][3] * b01;
  adjoint[1][3] = a[2][0] * b05 - a[2][2] * b02 + a[2][3] * b01;
  adjoint[2][0] = a[1][0] * b10 - a[1][1] * b08 + a[1][3] * b06;
  adjoint[2][1] = a[0][1] * b08 - a[0][0] * b10 - a[0][3] * b06;
  adjoint[2][2] = a[3][0] * b04 - a[3][1] * b02 + a[3][3] * b00;
  adjoint[2][3] = a[2][1] * b02 - a[2][0] * b04 - a[2][3] * b00;
  adjoint[3][0] = a[1][1] * b07 - a[1][0] * b09 - a[1][2] * b06;
  adjoint[3][1] = a[0][0] * b09 - a[0][1] * b07 + a[0][2] * b06;
  adjoint[3][2] = a[3][1] * b01 - a[3][0] * b03 - a[3][2] * b00;
  adjoint[3][3] = a[2][0] * b03 - a[2][1] * b01 + a[2][2] * b00;
  float determinant = b00 * b11 - b01 * b10 + b02 * b09 + b03 * b08 - b04 * b07 + b05 * b06;
  /* Multiplying by inverse since matrix types don't have divide operators. */
  return adjoint * (1.0 / determinant);
}

float3x3 inverse(float3x3 m)
{
  float3x3 adjoint;
  adjoint[0][0] = +(m[1][1] * m[2][2] - m[2][1] * m[1][2]);
  adjoint[0][1] = -(m[0][1] * m[2][2] - m[2][1] * m[0][2]);
  adjoint[0][2] = +(m[0][1] * m[1][2] - m[1][1] * m[0][2]);
  adjoint[1][0] = -(m[1][0] * m[2][2] - m[2][0] * m[1][2]);
  adjoint[1][1] = +(m[0][0] * m[2][2] - m[2][0] * m[0][2]);
  adjoint[1][2] = -(m[0][0] * m[1][2] - m[1][0] * m[0][2]);
  adjoint[2][0] = +(m[1][0] * m[2][1] - m[2][0] * m[1][1]);
  adjoint[2][1] = -(m[0][0] * m[2][1] - m[2][0] * m[0][1]);
  adjoint[2][2] = +(m[0][0] * m[1][1] - m[1][0] * m[0][1]);
  float determinant = m[0][0] * adjoint[0][0] + m[1][0] * adjoint[0][1] + m[2][0] * adjoint[0][2];
  /* Multiplying by inverse since matrix types don't have divide operators. */
  return adjoint * (1.0 / determinant);
}

float2x2 inverse(float2x2 m)
{
  float2x2 adjoint;
  adjoint[0][0] = +m[1][1];
  adjoint[1][0] = -m[1][0];
  adjoint[0][1] = -m[0][1];
  adjoint[1][1] = +m[0][0];
  float determinant = m[0][0] * m[1][1] - m[1][0] * m[0][1];
  /* Multiplying by inverse since matrix types don't have divide operators. */
  return adjoint * (1.0 / determinant);
}

/* Additional overloads for builtin functions. */
float distance(float x, float y)
{
  return abs(y - x);
}

/* Overload for mix(vec<T>, vec<T>, float). */
template<typename T, int S> vec<T, S> mix(vec<T, S> a, vec<T, S> b, float fac)
{
  return a * (1.0 - fac) + b * fac;
}

/* Using vec<bool, S> does not appear to work, splitting cases. */
#define SELECT \
  { \
    return select(a, b, mask); \
  }

template<typename T> vec<T, 4> mix(vec<T, 4> a, vec<T, 4> b, bool4 mask) SELECT;
template<typename T> vec<T, 3> mix(vec<T, 3> a, vec<T, 3> b, bool3 mask) SELECT;
template<typename T> vec<T, 2> mix(vec<T, 2> a, vec<T, 2> b, bool2 mask) SELECT;

#undef SELECT

template<typename T, int Size> bool is_zero(vec<T, Size> a)
{
  return all(a == vec<T, Size>(0));
}

#define in
#define out thread
#define inout thread
#define _in_sta
#define _in_end
#define _out_sta (&
#define _out_end )
#define _inout_sta (&
#define _inout_end )

#define shared threadgroup
#define _shared_sta (&
#define _shared_end )

/* Defines generated by glsl_preprocess that contains threadgroup variables related codegen.
 * See glsl_preprocess for more detail. */
#define MSL_SHARED_VARS_ARGS
#define MSL_SHARED_VARS_ASSIGN
#define MSL_SHARED_VARS_DECLARE
#define MSL_SHARED_VARS_PASS

/* Matrix reshaping functions. */
#define RESHAPE(mat_to, mat_from, ...) \
  mat_to to_##mat_to(mat_from m) \
  { \
    return mat_to(__VA_ARGS__); \
  }

/* clang-format off */
RESHAPE(float2x2, float3x3, m[0].xy, m[1].xy)
RESHAPE(float2x2, float4x4, m[0].xy, m[1].xy)
RESHAPE(float3x3, float4x4, m[0].xyz, m[1].xyz, m[2].xyz)
RESHAPE(float3x3, float2x2, m[0].x, m[0].y, 0, m[1].x, m[1].y, 0, 0, 0, 1)
RESHAPE(float4x4, float2x2, m[0].x, m[0].y, 0, 0, m[1].x, m[1].y, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1)
RESHAPE(float4x4, float3x3, m[0].x, m[0].y, m[0].z, 0, m[1].x, m[1].y, m[1].z, 0, m[2].x, m[2].y, m[2].z, 0, 0, 0, 0, 1)
/* clang-format on */
/* TODO(fclem): Remove. Use Transform instead. */
RESHAPE(float3x3, float3x4, m[0].xyz, m[1].xyz, m[2].xyz)
#undef RESHAPE

#undef ENABLE_IF

/* Array syntax compatibility. */
/* clang-format off */
#define float_array(...) { __VA_ARGS__ }
#define float2_array(...) { __VA_ARGS__ }
#define float3_array(...) { __VA_ARGS__ }
#define float4_array(...) { __VA_ARGS__ }
#define int_array(...) { __VA_ARGS__ }
#define int2_array(...) { __VA_ARGS__ }
#define int3_array(...) { __VA_ARGS__ }
#define int4_array(...) { __VA_ARGS__ }
#define uint_array(...) { __VA_ARGS__ }
#define uint2_array(...) { __VA_ARGS__ }
#define uint3_array(...) { __VA_ARGS__ }
#define uint4_array(...) { __VA_ARGS__ }
#define bool_array(...) { __VA_ARGS__ }
#define bool2_array(...) { __VA_ARGS__ }
#define bool3_array(...) { __VA_ARGS__ }
#define bool4_array(...) { __VA_ARGS__ }
#define ARRAY_T(type)
#define ARRAY_V(...) {__VA_ARGS__}
/* clang-format on */

#define SHADER_LIBRARY_CREATE_INFO(a)
#define VERTEX_SHADER_CREATE_INFO(a)
#define FRAGMENT_SHADER_CREATE_INFO(a)
#define COMPUTE_SHADER_CREATE_INFO(a)

#define _enum_type(name) name
#define _enum_decl(name) enum name {
#define _enum_end \
  } \
  ;
